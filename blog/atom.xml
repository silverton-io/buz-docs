<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://buz.dev/blog</id>
    <title>Buz Blog</title>
    <updated>2023-02-22T00:00:00.000Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://buz.dev/blog"/>
    <subtitle>Buz Blog</subtitle>
    <icon>https://buz.dev/img/favicon.ico</icon>
    <entry>
        <title type="html"><![CDATA[Serverless Makes Streaming Accessible]]></title>
        <id>/serverless-makes-streaming-accessible</id>
        <link href="https://buz.dev/blog/serverless-makes-streaming-accessible"/>
        <updated>2023-02-22T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Streaming systems have long been inaccessible to many companies, simply due to complexity. Serverless technologies make streaming accessible and will completely change data processing as we know it.]]></summary>
        <content type="html"><![CDATA[<p><a href="https://snowplow.io/" target="_blank" rel="noopener noreferrer">Snowplow Analytics</a> is a highly-scalable system that powers <a href="https://snowplow.io/blog/why-data-contracts-are-obviously-a-good-idea/" target="_blank" rel="noopener noreferrer">structured data creation</a> for <a href="https://trends.builtwith.com/analytics/Snowplow" target="_blank" rel="noopener noreferrer">millions of sites</a> on the internet. Snowplow tracking is incorporated into <a href="https://github.com/dbt-labs/dbt-core/blob/main/core/dbt/tracking.py#L33-L47" target="_blank" rel="noopener noreferrer">dbt</a>, <a href="https://cloud.getdbt.com/" target="_blank" rel="noopener noreferrer">dbt cloud</a>, <a href="https://trello.com/" target="_blank" rel="noopener noreferrer">Trello</a>, <a href="https://gitlab.com/" target="_blank" rel="noopener noreferrer">Gitlab</a>, <a href="https://www.citi.com/" target="_blank" rel="noopener noreferrer">Citi bank</a>, <a href="https://www.backcountry.com/" target="_blank" rel="noopener noreferrer">Backcountry.com</a>, and the list goes on.</p><p>After setting up data infrastructure <a href="https://bostata.com/268-billion-events-with-snowplow-snowflake-at-cargurus" target="_blank" rel="noopener noreferrer">like Snowplow</a> for <a href="https://bostata.com/client-side-instrumentation-for-under-one-dollar" target="_blank" rel="noopener noreferrer">years</a>  I've frequently found myself wishing for both <strong>less</strong> and <strong>more</strong>.</p><p><strong>Fewer streams</strong>, <strong>fewer machines or containers to manage</strong>, <strong>fewer moving pieces to help prevent event duplication or loss</strong>, <strong>less configuration</strong>, and <strong>less in-house documentation to keep things running</strong> would be a dream.</p><p><strong>Deployment flexibility</strong>, <strong>flexible schema storage</strong>, <strong>cost efficiencies</strong>, <strong>seamless migration between transport systems</strong>, <strong>improved utility from the data in transit</strong>, and <strong>increased visibility</strong> would also be very helpful.</p><p>Meanwhile, serverless technologies have come into their own and point the way toward a very bright data-processing future. Which is how <strong><a href="https://buz.dev" target="_blank" rel="noopener noreferrer">buz.dev</a></strong> was born.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="inspirations-and-iterations">Inspirations and iterations<a class="hash-link" href="#inspirations-and-iterations" title="Direct link to heading">‚Äã</a></h2><p>My first iteration of "serverless Snowplow Analytics" was in 2018.  I stitched together a <a href="https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-working-with.html" target="_blank" rel="noopener noreferrer">Cloudfront distribution</a>, a <a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html" target="_blank" rel="noopener noreferrer">Python-based Lambda function</a>, S3, and the <a href="https://www.jsdelivr.com/package/npm/@snowplow/javascript-tracker?path=dist" target="_blank" rel="noopener noreferrer">Snowplow javascript tracker</a> to stockpile event data. A series of <a href="https://docs.aws.amazon.com/athena/latest/ug/creating-tables.html" target="_blank" rel="noopener noreferrer">Athena external tables</a> sat on top of the raw data and voila! Near-real-time analytics for practically free. It worked so well <a href="https://bostata.com/client-side-instrumentation-for-under-one-dollar/" target="_blank" rel="noopener noreferrer">a blog post was written</a> and other people were inspired to <a href="https://discourse.snowplow.io/t/snowplow-serverless/1912/14" target="_blank" rel="noopener noreferrer">build</a> or <a href="https://www.ownyourbusinessdata.net/enrich-snowplow-data-with-aws-lambda-function/" target="_blank" rel="noopener noreferrer">write about</a> the same. The system ran hands-off with very little effort on my part, thanks to <strong>minimal moving pieces</strong> and <strong>AWS having the responsibility of keeping it up and running</strong>. </p><p>Serverless event collection worked very well at scale during my time at <a href="https://www.cargurus.com/" target="_blank" rel="noopener noreferrer">CarGurus</a>. While we had a Snowplow Analytics implementation, a colleage wanted to see what AWS Lambda could handle. The marketing team sent enormous amounts of email using a combination of <a href="https://iterable.com/" target="_blank" rel="noopener noreferrer">Iterable</a> and <a href="https://help.dyn.com/email-delivery-gsg/" target="_blank" rel="noopener noreferrer">Dyn</a>, and each blast would result in production systems being absolutely swamped with tracking callbacks. The analytics team wanted data for analytics or tracking opt-outs, but nobody wanted to provision (normally unused) static infrastructure. Lambda was a perfect fit. The system continuously ramped from 0 to ~20k rps and back again, the AWS bill was laughably small, and it required virtually zero maintenance.</p><p>After setting up Snowplow for a <a href="https://www.bisnow.com/" target="_blank" rel="noopener noreferrer">NYC-based commercial real estate company</a>, the VP of Technology (who is now @ Disney Streaming) pushed my implementation further with a simple Lambda function. Instead of being limited to Snowplow protocol payloads, the Lambda function collected arbitrary json payloads, reformatted them as Snowplow <a href="https://docs.snowplow.io/docs/understanding-tracking-design/out-of-the-box-vs-custom-events-and-entities/#self-describing-events" target="_blank" rel="noopener noreferrer">Self-Describing Events</a>, and fired them into the Snowplow collector. It was efficient and effectively hands-off. I have thought about his work ever since.</p><p>The rich benefits of serverless data processing for robotics and fulfillment were experienced while working at <a href="https://6river.com/data-driven-robotics-leveraging-google-cloud-platform-and-big-data-to-improve-robot-behaviors/" target="_blank" rel="noopener noreferrer">6 River Systems</a> (Shopify Logistics). Data volumes from fulfillment systems are highly variable. One warehouse or distribution center has a very different traffic pattern than another, but volume across all of them spikes to orders of magnitudes higher <a href="https://supplychaingamechanger.com/strategies-to-survive-the-peak-season-fulfillment-surge/" target="_blank" rel="noopener noreferrer">during Peak months</a>. Over-provisioned infrastructure in an industry where margins are already tight is a complete non-starter. Serverless was the only option and did not disappoint.</p><p>And finally, my time working alongside Okta's security team has thoroughly solidified the value of serverless technologies in a security-conscious setting. <em>Security teams absolutely love serverless tech.</em> Want proof? Go check out <a href="https://www.matano.dev/" target="_blank" rel="noopener noreferrer">Matano.dev</a>, ask <a href="https://panther.com/" target="_blank" rel="noopener noreferrer">Panther</a> how their systems are built, or dig into <a href="https://aws.amazon.com/blogs/publicsector/how-public-sector-security-teams-can-use-serverless-technologies-improve-outcomes/" target="_blank" rel="noopener noreferrer">AWS marketing materials</a>.</p><p><strong>Serverless is the secret of highly efficient data processing. It is the way to make streaming accessible.</strong></p><h1>Why Build Buz?</h1><p>While incredibly scalable and robust, setting up and maintaining OSS Snowplow is not for the faint of heart. It's time-consuming to set up, requires a deep understanding of the moving pieces to tune well, and requires significant engineering resources.</p><p>A very common Snowplow architecture diagram looks like the following, excluding monitoring, alerting, log centralization, and other devops necessities:</p><p><img loading="lazy" alt="snowplow" src="/assets/images/snowplow_arch-5bce4a2a8671352a98ff0d0ada048a7e.png" width="2431" height="1507" class="img_ev3q"></p><p>Opportunity costs matter to cost-conscious buinesses, and engineering resources dedicated to maintaining data pipelines are rarely the best use of said resources. <strong><em>Engineers also prefer to spend their time using streaming data rather than moving it around</em></strong>.</p><p>So my initial goal was to build a system like the following:</p><p><img loading="lazy" alt="serverless thing" src="/assets/images/initial_arch-aa272e4149d79bb767b2a3e12fa7f849.png" width="6586" height="833" class="img_ev3q"></p><p>Snowplow tracking SDK's would be used for instrumentation. The serverless collector thing called Buz would collect, validate, and route payloads to S3 via Kinesis Firehose. And Snowflake would provide the compute on top of S3.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="buz-requirements-and-design">Buz Requirements and Design<a class="hash-link" href="#buz-requirements-and-design" title="Direct link to heading">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="minimal-human-involvement-to-keep-running">Minimal human involvement to keep running<a class="hash-link" href="#minimal-human-involvement-to-keep-running" title="Direct link to heading">‚Äã</a></h3><p>Systems are great when you don't need to think about them. In <a href="https://twitter.com/b0rk" target="_blank" rel="noopener noreferrer">Julia Evans'</a> words, spending <strong><a href="https://jvns.ca/blog/2022/07/09/monitoring-small-web-services/" target="_blank" rel="noopener noreferrer">approximately 0 time on operations</a></strong> was the goal.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="self-contained-and-capable-of-running-horizontally-with-no-issue">Self-contained and capable of running horizontally with no issue<a class="hash-link" href="#self-contained-and-capable-of-running-horizontally-with-no-issue" title="Direct link to heading">‚Äã</a></h3><p>There's a movement of "small, mighty, and self-contained" afoot within data processing systems.</p><p><em>It's because complexity is hard to keep running</em>.</p><p>Systems like <a href="https://redpanda.com/" target="_blank" rel="noopener noreferrer">Redpanda</a>, which crams the Kafka api into a small self-contained binary, or <a href="https://www.benthos.dev/" target="_blank" rel="noopener noreferrer">Benthos</a>, which crams cool stream-processing functionality into a small self-contained binary, are highly inspirational.</p><p>The Serverless Thing called Buz needed to do the same.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="no-jvm-no-spark-no-beam">No JVM, no Spark, no Beam<a class="hash-link" href="#no-jvm-no-spark-no-beam" title="Direct link to heading">‚Äã</a></h3><p>Snowplow's <a href="https://docs.snowplow.io/docs/pipeline-components-and-applications/stream-collector/" target="_blank" rel="noopener noreferrer">collector</a>, <a href="https://docs.snowplow.io/docs/pipeline-components-and-applications/enrichment-components/enrich/#enrich-kinesis" target="_blank" rel="noopener noreferrer">enricher</a>, <a href="https://docs.snowplow.io/docs/pipeline-components-and-applications/loaders-storage-targets/s3-loader/" target="_blank" rel="noopener noreferrer">s3 sink</a>, etc all run on the JVM.</p><p>Snowplow's <a href="https://docs.snowplow.io/docs/pipeline-components-and-applications/loaders-storage-targets/snowplow-rdb-loader-3-0-0/" target="_blank" rel="noopener noreferrer">RDB</a> and <a href="https://docs.snowplow.io/docs/pipeline-components-and-applications/loaders-storage-targets/snowplow-snowflake-loader/" target="_blank" rel="noopener noreferrer">Snowflake</a> loaders run on Spark while the <a href="https://docs.snowplow.io/docs/pipeline-components-and-applications/loaders-storage-targets/bigquery-loader/" target="_blank" rel="noopener noreferrer">BigQuery</a> loader runs on Beam (Cloud Dataflow).</p><p>But... Snowflake's <a href="https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro.html" target="_blank" rel="noopener noreferrer">Snowpipe</a> works well, as do BigQuery <a href="https://cloud.google.com/bigquery/docs/samples/bigquery-table-insert-rows" target="_blank" rel="noopener noreferrer">streaming inserts</a> or  <a href="https://cloud.google.com/pubsub/docs/bigquery" target="_blank" rel="noopener noreferrer">Pub/Sub subscriptions</a>. And the responsibility of keeping Snowpipe or BQ streaming inserts running is offloaded :).</p><p>Serverless Thing was to shed as many dependencies as possible.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="fast-startup-and-shutdown">Fast startup and shutdown<a class="hash-link" href="#fast-startup-and-shutdown" title="Direct link to heading">‚Äã</a></h3><p>Making containers fast to launch makes a big impact on cost as invocations ramp, so Serverless Thing had to be snappy. The faster infrastructure can follow the utilization curve, the more cost-effective it is. In an environment where costs are being scrutinized, <strong>doing work fast</strong> is just as important as <strong>not running at all when there is no work to be done</strong>.</p><p>Being efficient also happens to be pretty damn good for the environment. Burning fewer polar bears seems to <a href="https://medium.com/@intive/this-months-reason-technology-will-save-the-world-energy-savings-and-serverless-principles-375660c8ed81" target="_blank" rel="noopener noreferrer">resonate with others</a> like <a href="https://youtu.be/Z-6SnP6yzgo?t=1826" target="_blank" rel="noopener noreferrer">DuckDB</a> and <a href="https://d39w7f4ix9f5s9.cloudfront.net/e3/79/42bf75c94c279c67d777f002051f/carbon-reduction-opportunity-of-moving-to-aws.pdf" target="_blank" rel="noopener noreferrer">451 Research</a>. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="payload-validation-annotation-and-bifurcation">Payload validation, annotation, and bifurcation<a class="hash-link" href="#payload-validation-annotation-and-bifurcation" title="Direct link to heading">‚Äã</a></h3><p>A very valuable Snowplow feature lies at the <code>Enricher</code>, where each and every event is validated using the associated jsonschema.</p><p>The only way to do this quickly is via a self-warming schema cache, so an onboard cache became another requirement.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="just-json">Just JSON<a class="hash-link" href="#just-json" title="Direct link to heading">‚Äã</a></h3><p>Snowplow data is serialized using <strong><a href="https://thrift.apache.org/" target="_blank" rel="noopener noreferrer">thrift</a></strong> between the collector and the enricher but becomes <strong>tsv</strong> downstream of the enricher. This makes it hard to point a system like <a href="https://materialize.com/" target="_blank" rel="noopener noreferrer">Materialize</a> at the "enriched" stream without first <code>reading tsv records</code> -&gt; <code>formatting as json</code> -&gt; <code>writing to a separate stream</code>. Write amplification quickly becomes reality and the operator must make a choice between <strong>not reading from the stream</strong> or <strong>re-formatting every payload to something that is easily pluggable with other stream processing systems</strong>. At higher volumes this equates to $$$$$.</p><p>While JSON is not the smallest data format it is still more efficient to write JSON once than having many copies of smaller formats. I chose to have <strong>fewer</strong> copies but a <strong>larger per-record format</strong>.</p><p>This decision is tbd. In the worst case it's easy to change to parquet depending on destination.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="easy-to-configure">Easy to configure<a class="hash-link" href="#easy-to-configure" title="Direct link to heading">‚Äã</a></h3><p>Yaml + Jsonschema validation is <a href="https://www.schemastore.org/json/" target="_blank" rel="noopener noreferrer">becoming pretty standard</a>. It turned out to be a pretty good decision since auto-completing, auto-validating config is handy.</p><p>Serverless Thing had to be easy to configure. Bonus points for providing hints in an editor throughout the configuration process.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="make-event-streaming-accessible">Make event streaming accessible<a class="hash-link" href="#make-event-streaming-accessible" title="Direct link to heading">‚Äã</a></h3><p><a href="https://www.getdbt.com/" target="_blank" rel="noopener noreferrer">dbt</a> has been so inspirational because it makes good data engineering practices accessible to all. Tricks that used to pay rent have become dbt packages anyone can import.</p><p>Like the data engineering of not-that-long-ago, today's streaming systems are <strong>intimidating</strong>. But they don't need to be. These systems are also often <strong>overkill</strong>. Transporting data via several streams only to batch-insert it into a Postgres database means the streaming infrastructure is unnecessary.</p><p>Serverless Thing should make streaming accessible, while making it easy to evolve from the current stack to some desired architecture. Even if that means shipping events to <a href="https://buz.dev/integrations/postgres" target="_blank" rel="noopener noreferrer">Postgres</a> now and <a href="https://buz.dev/integrations/kafka" target="_blank" rel="noopener noreferrer">Kafka</a> later.</p><h1>Progress thus far</h1><p>While Buz has much further to go, the journey of serverless event tracking has already been extremely worthwhile.</p><p>The pain and complexity of streaming systems seems to resonate with <strong><em>many</em></strong> people. <strong>Serverless fixes this pain.</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="expanding-to-more-inputs">Expanding to more inputs<a class="hash-link" href="#expanding-to-more-inputs" title="Direct link to heading">‚Äã</a></h3><p>I had a eureka moment early on - if the serverless model works using <a href="https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/snowplow-tracker-protocol/" target="_blank" rel="noopener noreferrer">Snowplow's tracker protocol</a> it should work for other protocols. As it turns out, it does! While also minimizing the hassle of running multiple event tracking pipelines - such as one pipeline for each protocol.</p><p><a href="/inputs/cloudNative/cloudevents">Cloudevents</a> with its (optional) <a href="https://cloudevents.github.io/sdk-javascript/interfaces/event_interfaces.CloudEventV1.html#dataschema" target="_blank" rel="noopener noreferrer">dataschema</a> property was a low-effort addition. Fire payloads using the Cloudevents' <code>data</code> property, provide a schema reference in <code>dataschema</code>, and voila! Validated events without needing to <a href="https://github.com/cloudevents?q=sdk-&amp;type=all&amp;language=&amp;sort=" target="_blank" rel="noopener noreferrer">write the sdk</a>, or quickly hooking into tracking already in existence.</p><p>Pixels and webhooks were fun additions, mostly because these are often painful due to the arbitrary nature of their payloads. But another thought came to mind - validate these too! Since it would be fab to namespace and validate these payloads, named <a href="/inputs/buz/pixel#named-pixels">pixels</a> and <a href="/inputs/buz/webhook#named-webhooks">webhooks</a> came to be.</p><p>Accepting <a href="/inputs/buz/self-describing">self-describing</a> json payloads was a nice addition, and provides some additional flexibility like custom top-level payload property names. It also makes internal SDK's a breeze to build.</p><p>In past work lives I've built lightweight sidecars to read <a href="https://www.mongodb.com/docs/manual/changeStreams/" target="_blank" rel="noopener noreferrer">Mongodb change streams</a> or <a href="https://www.postgresql.org/docs/current/logical-replication.html" target="_blank" rel="noopener noreferrer">Postgres logical replication</a> before writing data to systems like Kafka. Serverless Thing naturally lends itself to supporting change data capture, or at least the weird cousin of what CDC looks like today. A lovely example of what could (and/or should) be ubiquitous is <a href="https://tech.devoted.com/avalanche-streaming-postgres-to-snowflake-130e8c477f07?gi=db8239b2a6ad" target="_blank" rel="noopener noreferrer">Devoted Health's Avalanche</a> project. </p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="writing-events-to-a-variety-of-destinations">Writing events to a variety of destinations<a class="hash-link" href="#writing-events-to-a-variety-of-destinations" title="Direct link to heading">‚Äã</a></h3><p><code>Kinesis</code>, <code>Kafka</code>, and <code>Pub/Sub</code> are commonly-supported streaming/transport mechanisms for data systems. All three options work well. But they are often <strong>overkill and debatably net-negative</strong> for:</p><ul><li>Companies with low data volumes or <a href="https://motherduck.com/blog/big-data-is-dead/" target="_blank" rel="noopener noreferrer">not big data</a></li><li>Companies that don't <a href="https://twitter.com/tayloramurphy/status/1625995802924445697" target="_blank" rel="noopener noreferrer">need or want legitimately-real-time data access</a></li><li>Non-production environments</li><li>Local development</li></ul><p>At CarGurus I learned the importance of making systems accessible to engineers, regardless of which environment they run in. Product engineers care about one thing: <strong>data gets to where it needs to be, wherever that is.</strong> They typically do not care about <strong>how it gets there</strong> or <strong>where it needs to be for downstream consumption</strong>.</p><p>At Shopify I learned the intricacies of building systems that can be deployed in single-tenant fashion as efficiently as multi-tenant.</p><p>Having flexibility to write data to a variety of systems is a requirement for all of the above, so Buz quickly expanded to support:</p><ul><li><strong><a href="/category/streaming-sinks">Streaming destinations</a></strong> which are best in production at scale.</li><li><strong><a href="/outputs/stream/kinesis-firehose">Streaming hybrids like Kinesis Firehose</a></strong>, because they are hands-off and incredibly powerful for building data lakes.</li><li><strong><a href="/category/database-sinks">Traditional databases</a></strong>, because most every company has one already.</li><li><strong><a href="/outputs/database/materialize">Streaming databases</a></strong>, because they are the future.</li><li><strong><a href="/category/timeseries-sinks">Timeseries databases</a></strong>, because they unlock some very interesting use cases.</li><li><strong><a href="/category/saas-sinks">Saas products</a></strong>, because product and engineering teams everywhere use them.</li><li><strong><a href="/category/message-broker-sinks">Message brokers</a></strong>, because they are well-loved and very useful.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="writing-events-to-multiple-destinations">Writing events to multiple destinations<a class="hash-link" href="#writing-events-to-multiple-destinations" title="Direct link to heading">‚Äã</a></h3><p>Shopify has a streaming model where <strong>events are written to Kafka for distribution to Datadog for observability and to the data lake.</strong> A secondary model is <strong>simultaneously writing product/marketing events to Amplitude for product analytics and to the data lake.</strong> After seeing how simple yet operationally powerful these are, I had a hard time ignoring them.</p><p>Migrating systems is a very common pain point as needs, volume, and organizations evolve. Migrating from <code>Postgres</code> to <code>Kafka</code> or <code>Kinesis</code> to <code>Kafka</code> are common patterns, and dual writing is the way to do this without blowing everything up. I've often wanted to simply add a configuration block instead of writing a new system that will be thrown away after migration.</p><p><strong>So Buz supports writing to more than one destination. </strong></p><p>There are tradeoffs here that must not be ignored, as the risk of one destination being unavailable goes up pretty quickly when the number of them increases. It's a worthwhile ops lever nonetheless.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="flexible-schema-registry-backends-including-using-the-destination-system-as-a-registry">Flexible schema registry backends (including using the destination system as a registry)<a class="hash-link" href="#flexible-schema-registry-backends-including-using-the-destination-system-as-a-registry" title="Direct link to heading">‚Äã</a></h3><p>I've often questioned why data processing systems rarely use the backend they ship data to for serving configuration, schemas, and other runtime resources. Many web apps do this - why don't data systems?</p><p>In the spirit of minimizing moving pieces, and because it's fun, Buz supports a variety of schema registry backends.</p><p>Interesting use cases this functionality unlocks include:</p><ul><li><strong>Streaming analytics with no streams.</strong> Using <a href="/outputs/database/materialize">Materialize</a> as the destination as well as the <a href="/blog/under-the-hood/registry/backends/database/materialize">schema registry</a> means streaming insights don't rely on much infrastructure.</li><li><strong>Using Postgres as the application database, the event database, and the schema cache.</strong> Introducing event tracking to existing systems has literally never been easier.</li><li><strong>Analytics without a database at all.</strong> Using GCS or S3 for the schema cache and the data lake means a database is not required to get database-like results.</li><li><strong>Seamless tie-in to existing streaming workflows.</strong> If a Kafka or Redpanda schema is already in place, perfect! Yet-another-piece-of-infrastructure‚Ñ¢ should not be necessary.</li></ul><p>These unlocks are incredibly exciting.</p><h1>Where do we go from here?</h1><p>Thanks to serverless tech, we are in the early innings of complete data infrastructure transformation.</p><p>The serverless-first data processing idea seemed crazy for a very long time, <strong>but it's definitely not crazy.</strong> Companies like <a href="https://modal.com/" target="_blank" rel="noopener noreferrer">Modal</a> and <a href="https://www.snowflake.com/powered-by/panther-labs/" target="_blank" rel="noopener noreferrer">Panther</a> have been built from the ground-up to power data-oriented serverless workloads, <a href="https://www.fivetran.com/blog/serverless-etl-with-cloud-functions" target="_blank" rel="noopener noreferrer">Fivetran</a> leverages serverless for <a href="https://fivetran.com/docs/functions" target="_blank" rel="noopener noreferrer">custom connectors</a>, DuckDB can be easily <a href="https://twitter.com/mim_djo" target="_blank" rel="noopener noreferrer">tossed into a serverless function</a>, database drivers are <a href="https://planetscale.com/blog/introducing-the-planetscale-serverless-driver-for-javascript" target="_blank" rel="noopener noreferrer">being retooled</a> for <a href="https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html" target="_blank" rel="noopener noreferrer">serverless workloads</a>, and the list goes on.</p><p>Serverless enables highly-efficient, secure, and low-footprint data workloads. It drastically lowers the complexity bar of data processing systems, and enables teams to spend less time on the boring stuff.</p><p>Buz will continue to be all-in on serverless because it makes streaming accessible.</p>]]></content>
        <author>
            <name>Jake</name>
            <uri>https://github.com/jakthom</uri>
        </author>
        <category label="serverless event tracking" term="serverless event tracking"/>
        <category label="serverless snowplow analytics" term="serverless snowplow analytics"/>
        <category label="google cloud run" term="google cloud run"/>
        <category label="pub/sub" term="pub/sub"/>
        <category label="bigquery" term="bigquery"/>
        <category label="postgres" term="postgres"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[The Contract-Powered Data Platform]]></title>
        <id>the-contract-powered-data-platform</id>
        <link href="https://buz.dev/blog/the-contract-powered-data-platform"/>
        <updated>2022-10-06T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[The contract-powered data platform is a step towards improving data quality, reducing organizational friction, and automating the toil data teams face. Here's what it looks like and how it works.]]></summary>
        <content type="html"><![CDATA[<p>Between <a href="https://6river.com/?utm_source=buz.dev&amp;utm_content=hiitsme" target="_blank" rel="noopener noreferrer">6 River Systems</a> and <a href="https://www.cargurus.com/" target="_blank" rel="noopener noreferrer">CarGurus</a>, a very significant amount of my time over the past five years has been dedicated to data platform automation, reducing cross-team friction, and improving data quality.</p><p>Schemas have played a critical role in the process; this post outlines <strong>the why and the how</strong>. But before diving straight<!-- --> into the role of schemas (er... contracts) let's talk data platforms.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="components-of-a-good-data-platform">Components of a Good Data Platform<a class="hash-link" href="#components-of-a-good-data-platform" title="Direct link to heading">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="instrumentation-and-integration">Instrumentation and Integration<a class="hash-link" href="#instrumentation-and-integration" title="Direct link to heading">‚Äã</a></h3><p>This one goes without saying. If data is not being emitted from source systems you won't have any data to play with.</p><p>If you don't have any data the rest of this post will not provide value. You also won't be able to complain about the price of Snowflake and might feel left out. </p><p>Instrumentation is pretty important. It's also a pretty huge PITA to wrangle, which is why <a href="https://segment.com/academy/collecting-data/how-to-create-a-tracking-plan/" target="_blank" rel="noopener noreferrer">tracking</a> <a href="https://amplitude.com/blog/create-tracking-plan" target="_blank" rel="noopener noreferrer">plans</a> <a href="https://www.avo.app/blog/what-is-a-tracking-plan-and-why-do-you-need-one" target="_blank" rel="noopener noreferrer">became</a> <a href="https://www.indicative.com/resource/data-tracking-plan/" target="_blank" rel="noopener noreferrer">a</a> <a href="https://www.trackingplan.com/" target="_blank" rel="noopener noreferrer">thing</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-pipeline">The pipeline<a class="hash-link" href="#the-pipeline" title="Direct link to heading">‚Äã</a></h3><p>Pipelines are either <code>batch</code> or <code>streaming</code>. There's a holy war between the two religions, but similar concepts apply to both.</p><p>Pipelines collect data and put it somewhere. Sometimes they mutate said data. That's really it.</p><p>The <em>best</em> pipelines:</p><ul><li>Move data reliably.</li><li>Annotate payloads with metadata such as provenance, <code>collected_at</code> timestamps, fingerprints, etc.</li><li>Generate stats to provide the operator with feedback.</li><li>Validate and bifurcate payloads, if you're lucky.</li><li>Know about and act on payload sensitivities - obfuscate, hash, tokenize, redact, redirect, etc.</li><li>Minimize moving pieces.</li><li>Don't spend all the CEO's üí∏üí∏üí∏ so they can afford that house in the Bahamas.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="storage-and-access">Storage and access<a class="hash-link" href="#storage-and-access" title="Direct link to heading">‚Äã</a></h3><p>Data has to be stored somewhere- preferably somewhere accessible.</p><p>Storage/access systems range from a wee little Postgres database, to Snowflake, to a data lake filled with Parquet fish and the <del>Loch Ness</del> Trino monster.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-discovery">Data Discovery<a class="hash-link" href="#data-discovery" title="Direct link to heading">‚Äã</a></h3><p>As things scale, pipelines/databases/data models often turn into something the James Webb <em>and</em> dbt can't stitch back together.</p><p>Data discoverability is super important when organizations are fragmented, or when you're new to the company, or when you forget stuff as I often do.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="observability-monitoring-and-alerting">Observability, Monitoring, and Alerting<a class="hash-link" href="#observability-monitoring-and-alerting" title="Direct link to heading">‚Äã</a></h3><p>Last, definitely not least, and unfortunately-rare... tools that tell the operator if things are broken.</p><p>These could be devops-y tools like Prometheus/Alertmanager/Grafana, pay-to-play tools like data quality/reliability platforms, or something dead-simple like load metadata tables and freshness checks.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="design-goals-of-a-good-data-platform">Design Goals of a Good Data Platform<a class="hash-link" href="#design-goals-of-a-good-data-platform" title="Direct link to heading">‚Äã</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="comply-with-rules">Comply with rules<a class="hash-link" href="#comply-with-rules" title="Direct link to heading">‚Äã</a></h3><p>While maybe not the case within the US (for us Non-Californians), data regulation and compliance is kind of a big deal. If you don't comply, <a href="https://www.wired.com/story/google-analytics-europe-austria-privacy-shield/" target="_blank" rel="noopener noreferrer">good</a> <a href="https://edpb.europa.eu/news/national-news/2022/italian-sa-bans-use-google-analytics-no-adequate-safeguards-data-transfers_en" target="_blank" rel="noopener noreferrer">luck</a>.</p><p>Compliance is becoming less of a <code>goal</code> and more of a <code>requirement</code>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="minimize-bad-data">Minimize bad data<a class="hash-link" href="#minimize-bad-data" title="Direct link to heading">‚Äã</a></h3><p>Bad data is expensive. It's expensive to move, it's expensive to store, it's expensive to keep track of, and it's expensive to work around. Not knowing data is bad is even more expensive.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="maximize-knowledge-of-what-the-system-is-doing">Maximize knowledge of what the system is doing<a class="hash-link" href="#maximize-knowledge-of-what-the-system-is-doing" title="Direct link to heading">‚Äã</a></h3><p>Good things come from a knowledge of what a system is doing and when it is doing it.</p><p>Only after measurement can you optimize cost.</p><p>Only after timing can you make things faster.</p><p>And only after seeing a system end-to-end can you cut out unnecessary intermediaries.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="minimize-friction-for-all-parties-involved">Minimize friction for all parties involved<a class="hash-link" href="#minimize-friction-for-all-parties-involved" title="Direct link to heading">‚Äã</a></h3><p>Data platforms should be a good experience for everyone. Which includes <strong>many</strong> more parties than just analytics engineers or analysts.</p><p>Parties who are critical to success include:</p><ul><li>The frontend engineers, who work with Javascript/Typescript and any number of frameworks </li><li>The backend engineers, who work with Python, Node, Java, Go, C++</li><li>The native/app engineers, who work with something like Swift, Flutter, React Native</li><li>The devops people, who like when they can write less Terraform.</li><li>The SRE people, who like when they can see what's going on without asking you. Because you'll probably be asleep when they try.</li><li>The data engineers, who are usually on the hook when data goes bad.</li><li>The analytics engineers, who like when <code>user_id</code> means <code>user_id</code>.</li><li>The analysts, who like when they can push value back to product engineers.</li><li>The financefolk, who will come after you when costs exponentially increase.</li><li>The businessfolk, who will also come after you when costs exponentially increase.</li></ul><p>While data mandates and a new breed of data-oriented law might sound lovely (or not), these mechanisms only benefit a couple of the above parties. Mandates don't work. Telling other people to have more responsibility, so you can have less, also doesn't work.</p><p>Want to get buy-in? Minimize friction. Want to increase adoption? Automate others' toil. Want sustainable systems? Reduce cognitive load.</p><p>Which brings us back to schematizing stuff.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-contract-powered-platform">The Contract-Powered Platform<a class="hash-link" href="#the-contract-powered-platform" title="Direct link to heading">‚Äã</a></h2><p>I'm going to go out on a limb and just say it -&gt; schemas are the nucleus of sustainable data platforms.</p><p>Schematizing data upfront is often initially discarded and seen as <a href="https://twitter.com/Mike_Kaminsky/status/1573430588958445569" target="_blank" rel="noopener noreferrer">unnecessary overhead</a> or a productivity drain. The idea is nixed in favor of the eventual chaos arbitrary json creates. But who cares about our hypothetical future selves - it's our current selves that matter. Let's dig in for the sake of science.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-empower-the-producer---consumer-relationship">Schemas empower the "producer" &lt;-&gt; "consumer" relationship<a class="hash-link" href="#schemas-empower-the-producer---consumer-relationship" title="Direct link to heading">‚Äã</a></h3><p>Let's think about the two ends of data systems for a second.</p><p>Engineers exist at the source, product analysts typically exist at the "destination", and a black box lies between:</p><p><img loading="lazy" alt="mystery data thing" src="/assets/images/mystery_data_thing-7cf9e8be611dbfd4fe512fe661919539.png" width="2446" height="1126" class="img_ev3q"></p><p>But when the mystery data thing is removed, the <code>engineer</code> &lt;-&gt; <code>analyst</code> dynamic actually looks more like this:</p><p><img loading="lazy" alt="engineers and analysts" src="/assets/images/engs_and_analysts-c5f9ea148a629098798d55194dd478f9.png" width="2500" height="1218" class="img_ev3q"></p><p>This working dynamic is pretty terrible for productivity. The two parties communicate, sometimes. There's a ton of friction and neither party is to blame. Data engineers and managers are asked to join the conversation, and implicit "contracts" are established in a Google doc that everyone will lose track of.</p><p>(Human-readable) schemas turn this dynamic into something that looks more like the following:</p><p><img loading="lazy" alt="declarative thing" src="/assets/images/declarativeThing-bcb8716354dd657813ffbe9f0cf061e1.png" width="2550" height="948" class="img_ev3q"></p><p>Both parties contribute to a schema with consistent verbiage, which is then leveraged to generate the equivalent data structure in their language of choice.</p><p>Today's data workflows look <strong>very similar</strong> to how software engineering looked prior to <a href="https://github.blog/2008-02-23-oh-yeah-there-s-pull-requests-now/" target="_blank" rel="noopener noreferrer">Github announcing Pull Requests in 2008</a>. They work, but aren't ideal.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-are-data-discovery">Schemas are data discovery<a class="hash-link" href="#schemas-are-data-discovery" title="Direct link to heading">‚Äã</a></h3><p>In LinkedIn's <a href="https://engineering.linkedin.com/blog/2020/datahub-popular-metadata-architectures-explained" target="_blank" rel="noopener noreferrer">popular metadata architectures explained</a>, pull-based metadata extraction is outlined as "an ok start". Push-based metadata is "ideal".</p><p>Schematizing data upfront means data discovery and documentation writes itself. Data assets are discoverable as soon as schemas are deployed - before the data actually starts flowing.</p><p>Schematization mechanisms like JSON Schema also get <a href="https://json-schema.org/specification.html#meta-schemas" target="_blank" rel="noopener noreferrer">pretty meta</a>, so it's easy to add annotation such as:</p><ul><li>"These properties contain PII"</li><li>"These properties should be tokenized"</li><li>"X person on Y team owns this schema"</li><li>"This is version <code>1.4</code>. Here's how this data has evolved from <code>1.0</code>."</li></ul><p>This class of metadata is a CISO's <em>dream</em>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-power-data-validation-in-transit">Schemas power data validation in transit<a class="hash-link" href="#schemas-power-data-validation-in-transit" title="Direct link to heading">‚Äã</a></h3><p>Comparing a payload to "what it was supposed to be" and annotating it with a simple üëçüëé is extremely valuable. Schemas are the "what it was supposed to be".</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-help-stop-bad-instrumentation-from-being-implemented-in-the-first-place">Schemas help stop bad instrumentation from being implemented in the first place<a class="hash-link" href="#schemas-help-stop-bad-instrumentation-from-being-implemented-in-the-first-place" title="Direct link to heading">‚Äã</a></h3><p>Another +1 (for engineers) is the fact schemas help prevent bad tracking from getting deployed in the first place.</p><p>Language-specific data structures can be generated from schemas. Which means intellisense or the compiler complains during development if required props are missing, or if one is a <code>string</code> and should be a <code>bool</code>. And then the code blows up again at compile time if the bug is still there.</p><p>Nobody likes being the person who causes the release train to halt. Or being the person who caused the rollback because a prop was missing. <strong>Especially when it's "just for analytics".</strong></p><p>Merging to <code>main</code> only after instrumentation is üëçüëç is the ideal workflow. It saves rollbacks. It avoids the human friction of going to the data team... <em>again</em>, to have them explain their mandated "contract".</p><p>And it's just good engineering.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-improve-code-quality">Schemas improve code quality<a class="hash-link" href="#schemas-improve-code-quality" title="Direct link to heading">‚Äã</a></h3><p>This might be a stretch. Or maybe not.</p><p>Have you tried forcing an engineer who loves Typescript to use <code>Any</code>, while simultaneously mandating payloads have <code>propA</code>, <code>propB</code>, and <code>propC</code>. And <code>propC</code> must be a <code>bool</code>?</p><p>Or tried forcing a golang-oriented engineer to use a <code>map[string]interface{}</code>, but told them the payload must have specific keys?</p><p>I have. And it was pretty silly. And a couple quick Google searches highlight <a href="https://medium.com/@warkiringoda/typescript-best-practices-2021-a58aee199661" target="_blank" rel="noopener noreferrer">Don't Use Any</a>. <a href="https://bitfieldconsulting.com/golang/commandments" target="_blank" rel="noopener noreferrer">Use <code>map[string]interface{}</code> sparingly</a>. Lint rules will not-so-nicely tell you to pound sand.</p><p>Schemas are centralized verbiage from which to generate language-specific data structures. Tools like <a href="https://github.com/quicktype/quicktype" target="_blank" rel="noopener noreferrer">Quicktype</a>, <a href="https://github.com/sinclairzx81/typebox" target="_blank" rel="noopener noreferrer">Typebox</a>, and <a href="https://www.npmjs.com/package/json-schema-to-typescript" target="_blank" rel="noopener noreferrer">jsonschema-to-typescript</a> make this a reality. The same can be said about <a href="https://www.rfc-editor.org/rfc/rfc8927" target="_blank" rel="noopener noreferrer">JTD</a> and <a href="https://developers.google.com/protocol-buffers" target="_blank" rel="noopener noreferrer">Protocol Buffers</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-power-automation">Schemas power automation<a class="hash-link" href="#schemas-power-automation" title="Direct link to heading">‚Äã</a></h3><p>Schemas make data engineering quality of life significantly better. Destination tables can be automatically created and migrated as schemas evolve. Kafka topics and Pub/Sub streams can be automatically provisioned using the schema namespace. A single stream can be fanned out to a <a href="https://docs.aws.amazon.com/firehose/latest/dev/dynamic-partitioning.html" target="_blank" rel="noopener noreferrer">dynamically-partitioned data lake</a>. And a whole lot more.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-as-observability">Schemas as observability<a class="hash-link" href="#schemas-as-observability" title="Direct link to heading">‚Äã</a></h3><p>Calculating namespace-level statistics and splicing them into observability tools is the natural next step.</p><p>Stakeholder FAQ's (long before actual analytics) commonly look like:</p><ul><li>"I just implemented tracking. Is the data flowing?"</li><li>"When was some.event.v1 first deployed?"</li><li>"Is some.event.v1 still flowing?"</li><li>"Are we seeing any bad events after most recent deploy?"</li><li>"How much data are we processing for schema x.y.z?"</li><li>"I just changed some javascript. Am I still emitting one event or has it become ten?"</li><li>"What team should I go ask about a.b.c?"</li><li>"How does this event get generated again?"</li></ul><p>When the name/namespace of a schema is present with each payload, and payloads are shipped to tools like Datadog, <strong>people can self-service answers to these questions.</strong></p><p>When the name/namespace of a schema is present with each payload, and the payloads are loaded into Snowflake, <strong>people can self-service answers to these questions.</strong></p><p>Self-service, low-cognitive-load systems minimize friction for everyone.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-power-compliance-oriented-requirements">Schemas power compliance-oriented requirements<a class="hash-link" href="#schemas-power-compliance-oriented-requirements" title="Direct link to heading">‚Äã</a></h3><p>(Actually) adhering to data privacy-oriented regulation requires a rethink of pretty much all data processing systems. The place to tokenize, redact, or hash personal information is not at the end of the data pipeline. It is at the start. Otherwise you'll have sensitive data lying all over S3 in cleartext or flying through Kafka with no auth, and zero clue how to actually find or mitigate it.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-are-the-foundation-of-higher-order-data-models">Schemas are the foundation of higher-order data models<a class="hash-link" href="#schemas-are-the-foundation-of-higher-order-data-models" title="Direct link to heading">‚Äã</a></h3><p>It is pretty easy to turn a schema into a <a href="https://docs.getdbt.com/docs/building-a-dbt-project/using-sources" target="_blank" rel="noopener noreferrer">dbt source</a> so analytics engineers can reliably build upon a well-defined, trustworthy foundation.</p><p>If the foundation is not strong the analytics engineering team will spend all their time building "layer 1 base models" to santize inputs. In non-professional settings this would be called <a href="https://www.youtube.com/watch?v=VoP1E9J4jpg" target="_blank" rel="noopener noreferrer">Whack A Mole</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-are-the-foundation-of-data-products">Schemas are the foundation of data products<a class="hash-link" href="#schemas-are-the-foundation-of-data-products" title="Direct link to heading">‚Äã</a></h3><p>Similar to data modeling benefits, schemas allow data products to be built on a solid foundation. But there's more that can be done on top of that foundation!</p><p>Automatically-generated endpoints, GraphQL queries, and API docs? Can do. Tools like <a href="https://quicktype.io/" target="_blank" rel="noopener noreferrer">Quicktype</a>, <a href="https://transform.tools/" target="_blank" rel="noopener noreferrer">Transform</a>, and <a href="https://www.apollographql.com/" target="_blank" rel="noopener noreferrer">Apollo</a> immediately come to mind. As does a blog post from the folks at <a href="https://wundergraph.com/blog/build_json_apis_with_json_schema_by_writing_graphql_operations_against_any_datasource_like_graphql_rest_apollo_federation_postgresql_mysql" target="_blank" rel="noopener noreferrer">Wundergraph</a>.</p><p>"Schemas at the center" is a pattern engineers are already comfortable with. <a href="https://www.openapis.org/" target="_blank" rel="noopener noreferrer">OpenAPI</a> is simply a declarative schema between API's&lt;-&gt;frontends after all.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-contract-powered-workflow">The Contract-Powered Workflow<a class="hash-link" href="#the-contract-powered-workflow" title="Direct link to heading">‚Äã</a></h2><p>This is the workflow I've settled on after years of flipping levers and seeing what works (and what doesn't). Mandates don't work. Making analytics teams happy at the expense of poor application code going into production doesn't work. Knowing instrumentation is bad only after it is deployed works-ish, but just barely. Would not recommend.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="draft-iterate-on-and-deploy-a-schema">Draft, iterate on, and deploy a schema.<a class="hash-link" href="#draft-iterate-on-and-deploy-a-schema" title="Direct link to heading">‚Äã</a></h3><p>The neat thing about schema-first workflows is <strong>non-engineer stakeholders can write the first draft</strong>. You don't have to be a Typescript guru to get the process going, though engineering counterparts will need to be involved eventually.</p><p>The more work that can be front-loaded to non-engineers the better. Everyone's time is valuable and schemas allow everyone to proactively contribute to the process. It sucks when useful contributions are discarded because they are "not in a usable format" (cough, Gdocs). Schemas are usable formats.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bring-tracking-libraries-and-systems-up-to-parity">Bring tracking libraries and systems up to parity.<a class="hash-link" href="#bring-tracking-libraries-and-systems-up-to-parity" title="Direct link to heading">‚Äã</a></h3><p>The second a new schema or updated version has been published, automation kicks in and (at minimum):</p><ul><li>Builds and deploys new tracking SDK's for engineering teams</li><li>Pushes schema metadata ‚àÜ to data discovery tools</li><li>Ensures infrastructure dependencies (Kafka topics, database tables, etc)</li><li>Pushes the schema to the appropriate place for pipeline-level validation</li><li>Creates dbt sources for the analytics engineers</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="implement-tracking">Implement tracking.<a class="hash-link" href="#implement-tracking" title="Direct link to heading">‚Äã</a></h3><p>Once systems are ready to accept the new instrumentation, engineers implement it into a codebase. It doesn't matter if this is frontend code, backend code, cli's, infrastructure tooling, etc - the process is the same.</p><p>Getting dependencies squared away takes a matter of seconds. By the time engineers are ready to implement the new tracking, the entire system is ready to go.</p><p>A question I've heard over and over from engineers is "how do I know these payloads are making it to where they need to be?" This question is best answered with "go check Datadog/Graylog/Whatever". And followed up with, "or you could also go check Snowflake for a table of the same name".</p><p>The faster engineers have feedback the better.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="deploy">Deploy.<a class="hash-link" href="#deploy" title="Direct link to heading">‚Äã</a></h3><p>And lastly - making tracking part of the codebase. A huge pain point of analytics-oriented instrumentation is the fact it's often identified as "bad" after being pushed into prod. This is not awesome, and it greatly contributes to the upstream "we'll just throw arbitrary json down the line" concensus. Everyone knows this is not ideal, but it's definitely better than rolling back every-other deploy due to analytics bugs.</p><p>With contract-powered workflows the following prereqs are taken care of <em>before</em> instrumentation rolls out, not after:</p><ul><li>Implementers and stakeholders talk to each other using shared verbiage.</li><li>Versioned, language-specific data structures are generated like all other code dependencies.</li><li>Metadata is pushed to discovery tools.</li><li>The pipeline is primed to accept incoming payloads and mark them as "good" or "bad".</li><li>Observability tools are ready to go for instantaneous feedback in development and production.</li><li>Downstream analytics/modeling entrypoints (like dbt sources) are in place and can be immediately used.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-schema-powered-future">The Schema-Powered Future<a class="hash-link" href="#the-schema-powered-future" title="Direct link to heading">‚Äã</a></h2><p>If it's not obvious by now, schemas are <strong>awesome</strong>.</p><p>These workflows have significantly improved my work life, I know they've improved my colleagues', and it's probably just a matter of time before they improve yours too.</p><p>The fun part is it feels like this ecosystem is just getting started, and there are <strong>so</strong> many additional implications for the better. It's not a new or original concept by any means. But as data management capabililities at Non-Google companies progress, it will be a consistent solution for consistent pain.</p><p><strong>Some other reading if you want to dive in:</strong></p><ul><li><a href="https://slack.engineering/data-wrangling-at-slack/" target="_blank" rel="noopener noreferrer">Data Wrangling at Slack</a></li><li><a href="https://www.slideshare.net/alexismidon/jitney-kafka-at-airbnb" target="_blank" rel="noopener noreferrer">Jitney at AirBnb</a></li><li><a href="https://engineering.linkedin.com/blog/2020/pegasus-data-language" target="_blank" rel="noopener noreferrer">Pegasus at LinkedIn</a></li><li><a href="https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2020/03/Schema-Integration-at-Uber-Scale-US2TS-2020-1.pdf" target="_blank" rel="noopener noreferrer">Dragon at Uber</a></li><li><a href="https://doordash.engineering/2022/08/02/building-scalable-real-time-event-processing-with-kafka-and-flink/" target="_blank" rel="noopener noreferrer">Building Scalable Real Time Event Processing with Kafka and Flink</a> at Doordash</li><li><a href="https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873" target="_blank" rel="noopener noreferrer">Data Mesh at Netflix</a></li><li><a href="https://shopify.engineering/real-time-buyer-signal-data-pipeline-shopify-inbox" target="_blank" rel="noopener noreferrer">Building a Real-time Buyer Signal Data Pipeline for Shopify Inbox</a></li></ul><p><strong>You could also:</strong></p><ul><li>Tune in for <a href="https://coalesce.getdbt.com/agenda/getting-jiggy-with-jsonschema-the-power-of-contracts-for-building-data-systems" target="_blank" rel="noopener noreferrer">Getting jiggy with JSON Schema at dbt Coalesce</a> or <a href="https://twitter.com/emilyhawkins__" target="_blank" rel="noopener noreferrer">join the conversation</a> on <a href="https://twitter.com/aerialfly" target="_blank" rel="noopener noreferrer">Twitter</a>.</li></ul><p>As closing context from a Shopify perspective, 9800+ schemas and 1800+ contributors (many of whom are not engineers) is a huge feat. As is deploying hundreds of schema-generated instrumentation blocks to thousands of robots around the world. The model works.</p><p>Here's to our schema-powered future ü•Ç.</p>]]></content>
        <author>
            <name>Jake</name>
            <uri>https://github.com/jakthom</uri>
        </author>
        <category label="data platform" term="data platform"/>
        <category label="schemas" term="schemas"/>
        <category label="interfaces" term="interfaces"/>
        <category label="data contracts or whatever" term="data contracts or whatever"/>
    </entry>
    <entry>
        <title type="html"><![CDATA[Make It Easy]]></title>
        <id>make-it-easy</id>
        <link href="https://buz.dev/blog/make-it-easy"/>
        <updated>2022-08-17T00:00:00.000Z</updated>
        <summary type="html"><![CDATA[Defining, implementing, and evolving events is hard.]]></summary>
        <content type="html"><![CDATA[<p>Defining, implementing, and evolving events is hard.</p><p>Running event collection systems is also hard.</p><p>Buz aims to make it easy.</p><p><strong><a href="/">What is Buz?</a></strong></p>]]></content>
        <author>
            <name>Jake</name>
            <uri>https://github.com/jakthom</uri>
        </author>
    </entry>
</feed>