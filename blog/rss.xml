<?xml version="1.0" encoding="utf-8"?>
<rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:content="http://purl.org/rss/1.0/modules/content/">
    <channel>
        <title>Buz Blog</title>
        <link>https://buz.dev/blog</link>
        <description>Buz Blog</description>
        <lastBuildDate>Thu, 06 Oct 2022 19:36:03 GMT</lastBuildDate>
        <docs>https://validator.w3.org/feed/docs/rss2.html</docs>
        <generator>https://github.com/jpmonette/feed</generator>
        <language>en</language>
        <item>
            <title><![CDATA[The Contract-Powered Data Platform]]></title>
            <link>https://buz.dev/blog/the-contract-powered-data-platform</link>
            <guid>the-contract-powered-data-platform</guid>
            <pubDate>Thu, 06 Oct 2022 19:36:03 GMT</pubDate>
            <description><![CDATA[Between 6 River Systems and CarGurus, a very significant amount of my time over the past five years has been dedicated to data platform automation, reducing cross-team friction, and improving data quality.]]></description>
            <content:encoded><![CDATA[<p>Between <a href="https://6river.com/?utm_source=buz.dev&amp;utm_content=hiitsme" target="_blank" rel="noopener noreferrer">6 River Systems</a> and <a href="https://www.cargurus.com/" target="_blank" rel="noopener noreferrer">CarGurus</a>, a very significant amount of my time over the past five years has been dedicated to data platform automation, reducing cross-team friction, and improving data quality.</p><p>Schemas have played a critical role in this process; this post outlines <strong>the why and the how</strong>. But before diving straight<!-- --> into the role of schemas (er... contracts) let's talk data platforms.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="components-of-a-good-data-platform">Components of a Good Data Platform<a class="hash-link" href="#components-of-a-good-data-platform" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="instrumentation-and-integration">Instrumentation and Integration<a class="hash-link" href="#instrumentation-and-integration" title="Direct link to heading">​</a></h3><p>This one pretty much goes without saying. If data is not being emitted from source systems, you won't have any data to play with.</p><p>If you don't have or want any data the rest of this post will not provide value. You also won't be able to complain about price of Snowflake and might feel left out. </p><p>Instrumentation is pretty important. It's also a pretty huge PITA to wrangle, which is why <a href="https://segment.com/academy/collecting-data/how-to-create-a-tracking-plan/" target="_blank" rel="noopener noreferrer">tracking</a> <a href="https://amplitude.com/blog/create-tracking-plan" target="_blank" rel="noopener noreferrer">plans</a> <a href="https://www.avo.app/blog/what-is-a-tracking-plan-and-why-do-you-need-one" target="_blank" rel="noopener noreferrer">became</a> <a href="https://www.indicative.com/resource/data-tracking-plan/" target="_blank" rel="noopener noreferrer">a</a> <a href="https://www.trackingplan.com/" target="_blank" rel="noopener noreferrer">thing</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="the-pipeline">The pipeline<a class="hash-link" href="#the-pipeline" title="Direct link to heading">​</a></h3><p>Pipelines are either <code>batch</code> or <code>streaming</code>. There's a holy war between the two religions, but similar concepts apply to both.</p><p>Pipelines collect data and put it somewhere. Sometimes they mutate said data. That's really it.</p><p>The <em>best</em> pipelines:</p><ul><li>Collect data reliably.</li><li>Annotate payloads with metadata such as provenance, <code>collected_at</code> timestamps, fingerprints, etc.</li><li>Generate stats to provide the operator with feedback.</li><li>Validate and bifurcate payloads, if you're lucky.</li><li>Know about and act on payload sensitivities - obfuscate, hash, tokenize, redact, redirect, etc.</li><li>Minimize moving pieces.</li><li>Don't spend all the CEO's 💸💸💸 so they can afford that house in the Bahamas.</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="storage-and-access">Storage and access<a class="hash-link" href="#storage-and-access" title="Direct link to heading">​</a></h3><p>Data has to be stored somewhere- preferably it's somewhere accessible.</p><p>Storage/access systems range from a wee little Postgres database, to Snowflake, to a data lake filled with Parquet fish and the <del>Loch Ness</del> Trino monster.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="data-discovery">Data Discovery<a class="hash-link" href="#data-discovery" title="Direct link to heading">​</a></h3><p>As things scale, pipelines/databases/data models typically turn into something the James Webb <em>and</em> dbt can't stitch back together.</p><p>Data discoverability is super important when organizations are fragmented, or when you're new to the company, or when you forget stuff as I often do.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="observability-monitoring-and-alerting">Observability, Monitoring, and Alerting<a class="hash-link" href="#observability-monitoring-and-alerting" title="Direct link to heading">​</a></h3><p>Last, definitely not least, and unfortunately-rare... tools that tell the operator if things are broken.</p><p>These could be devops-y tools like Prometheus/Alertmanager/Grafana, pay-to-play tools like data quality/reliability platforms, or something dead-simple like load metadata tables and freshness checks.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="design-goals-of-a-good-data-platform">Design Goals of a Good Data Platform<a class="hash-link" href="#design-goals-of-a-good-data-platform" title="Direct link to heading">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="comply-with-rules">Comply with rules<a class="hash-link" href="#comply-with-rules" title="Direct link to heading">​</a></h3><p>While maybe not the case within the US (for us Non-Californians), data regulation and compliance is kind of a big deal. If you don't comply, <a href="https://www.wired.com/story/google-analytics-europe-austria-privacy-shield/" target="_blank" rel="noopener noreferrer">good</a> <a href="https://edpb.europa.eu/news/national-news/2022/italian-sa-bans-use-google-analytics-no-adequate-safeguards-data-transfers_en" target="_blank" rel="noopener noreferrer">luck</a>.</p><p>Compliance is becoming less of a <code>goal</code> and more of a <code>requirement</code>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="minimize-bad-data">Minimize bad data<a class="hash-link" href="#minimize-bad-data" title="Direct link to heading">​</a></h3><p>Bad data is expensive. It's expensive to move, it's expensive to store, it's expensive to keep track of, and it's expensive to work around. Not knowing data is bad is even more expensive.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="maximize-knowledge-of-what-the-system-is-doing">Maximize knowledge of what the system is doing<a class="hash-link" href="#maximize-knowledge-of-what-the-system-is-doing" title="Direct link to heading">​</a></h3><p>Good things come from a knowledge of what a system is doing and when it is doing it.</p><p>Only after measurement can you optimize cost.</p><p>Only after timing can you make things faster.</p><p>And only after seeing a system end-to-end can you cut out unnecessary intermediaries.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="minimize-friction-for-all-parties-involved">Minimize friction for all parties involved<a class="hash-link" href="#minimize-friction-for-all-parties-involved" title="Direct link to heading">​</a></h3><p>Data platforms should be a good experience for anyone who is involved, which includes <strong>many</strong> more parties than just analytics engineers or analysts.</p><p>Parties who are critical to success include:</p><ul><li>The frontend engineers, who work with Javascript/Typescript and any number of frameworks </li><li>The backend engineers, who work with Python, Node, Java, Go, C++</li><li>The native/app engineers, who work with something like Swift, Flutter, React Native</li><li>The devops people, who like when they can write less Terraform</li><li>The SRE people, who like when they can see what's going on without asking you. Because you'll probably be asleep when they try.</li><li>The data engineers, who are usually on the hook when data goes bad.</li><li>The analytics engineers, who like when <code>user_id</code> means <code>user_id</code>.</li><li>The analysts, who like when they can push value back to product engineers.</li><li>The financefolk, who will come after you when costs exponentially increase.</li><li>The businessfolk, who will also come after you when costs exponentially increase.</li></ul><p>While data mandates and a new breed of data-oriented law might sound lovely (or not), these mechanisms only benefit a couple of the above parties. Mandates don't work. Telling other people to have more responsibility, so you can have less, also doesn't work.</p><p>Want to get buy-in? Minimize friction. Want to increase adoption? Automate others' toil. Want sustainable systems? Reduce cognitive load.</p><p>Which brings us back to schematizing stuff.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-contract-powered-platform">The Contract-Powered Platform<a class="hash-link" href="#the-contract-powered-platform" title="Direct link to heading">​</a></h2><p>I'm going to go out on a limb and just say it -&gt; schemas are the nucleus of sustainable data platforms.</p><p>Schematizing data upfront is often initially discarded and seen as <a href="https://twitter.com/Mike_Kaminsky/status/1573430588958445569" target="_blank" rel="noopener noreferrer">unnecessary overhead</a> or a productivity drain. The idea is nixed in favor of the eventual chaos arbitrary json creates. But who cares about our hypothetical future selves - it's our current selves that matter. Let's dig in for the sake of science.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-empower-the-producer---consumer-relationship">Schemas empower the "producer" &lt;-&gt; "consumer" relationship<a class="hash-link" href="#schemas-empower-the-producer---consumer-relationship" title="Direct link to heading">​</a></h3><p>Let's think about the two ends of data systems for a second.</p><p>Engineers exist at the source, product analysts typically exist at the "destination", and a black box lies between:</p><p><img loading="lazy" alt="mystery data thing" src="/assets/images/mystery_data_thing-7cf9e8be611dbfd4fe512fe661919539.png" width="2446" height="1126" class="img_ev3q"></p><p>But when the mystery data thing is removed, the <code>engineer</code> &lt;-&gt; <code>analyst</code> dynamic actually looks more like this:</p><p><img loading="lazy" alt="engineers and analysts" src="/assets/images/engs_and_analysts-c5f9ea148a629098798d55194dd478f9.png" width="2500" height="1218" class="img_ev3q"></p><p>This working dynamic is pretty terrible for productivity. The two parties communicate, sometimes. There's a ton of friction and neither party is to blame. Data engineers and managers are asked to join the conversation, and implicit "contracts" are established in a Google doc that everyone will lose track of.</p><p>(Human-readable) schemas turn this dynamic into something that looks more like the following:</p><p><img loading="lazy" alt="declarative thing" src="/assets/images/declarativeThing-bcb8716354dd657813ffbe9f0cf061e1.png" width="2550" height="948" class="img_ev3q"></p><p>Both parties are able to contribute to the shared schema using the same "language", and the schema is then leveraged to generate the equivalent representation in their language of choice.</p><p>Today's data workflows look <strong>very similar</strong> to how software engineering looked prior to <a href="https://github.blog/2008-02-23-oh-yeah-there-s-pull-requests-now/" target="_blank" rel="noopener noreferrer">Github announcing Pull Requests in 2008</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-are-data-discovery">Schemas are data discovery<a class="hash-link" href="#schemas-are-data-discovery" title="Direct link to heading">​</a></h3><p>In LinkedIn's <a href="https://engineering.linkedin.com/blog/2020/datahub-popular-metadata-architectures-explained" target="_blank" rel="noopener noreferrer">popular metadata architectures explained</a>, pull-based metadata extraction is outlined as "an ok start". Push-based metadata is "ideal".</p><p>Schematizing data upfront means data discovery and documentation writes itself. Data assets are discoverable as soon as schemas are deployed - before the data actually starts flowing.</p><p>Schematization mechanisms like JSON Schema also get <a href="https://json-schema.org/specification.html#meta-schemas" target="_blank" rel="noopener noreferrer">pretty meta</a>, so it's easy to add schema-level metadata:</p><ul><li>"These properties contain PII"</li><li>"These properties should be tokenized"</li><li>"X person on Y team owns this schema"</li><li>"This is version <code>1.4</code>. Here's how this data has evolved from <code>1.0</code>."</li></ul><p>This class of metadata is a CISO's <em>dream</em>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-power-data-validation-in-transit">Schemas power data validation in transit<a class="hash-link" href="#schemas-power-data-validation-in-transit" title="Direct link to heading">​</a></h3><p>Comparing a payload to "what it was supposed to be" and annotating it with a simple 👍👎 is extremely valuable. Schemas are the "what it was supposed to be".</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-help-stop-bad-instrumentation-from-being-implemented-in-the-first-place">Schemas help stop bad instrumentation from being implemented in the first place<a class="hash-link" href="#schemas-help-stop-bad-instrumentation-from-being-implemented-in-the-first-place" title="Direct link to heading">​</a></h3><p>Another +1 (for engineers) is the fact schemas help bad tracking from getting deployed in the first place.</p><p>Language-specific data structures can be generated from schemas. Which means intellisense or the compiler complains during development if required props are missing, or if one is a <code>string</code> and should be a <code>bool</code>. And then the code blows up again at compile time if the bug is still there.</p><p>Nobody likes being the person who causes the release train to halt. Or being the person who caused the rollback because a prop was missing. <strong>Especially when it's "just for analytics".</strong></p><p>Merging to <code>main</code> only after instrumentation is 👍👍 is the ideal workflow. It saves rollbacks. It avoids the human friction of going to the data team... <em>again</em>, to have them explain their mandated "contract". And it's just good engineering.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-improve-code-quality">Schemas improve code quality<a class="hash-link" href="#schemas-improve-code-quality" title="Direct link to heading">​</a></h3><p>This might be a stretch. Or maybe not.</p><p>Have you tried forcing an engineer who loves Typescript to use <code>Any</code>, while simultaneously mandating payloads have <code>propA</code>, <code>propB</code>, and <code>propC</code>. And <code>propC</code> must be a <code>bool</code>?</p><p>Or tried forcing a golang-oriented engineer to use a <code>map[string]interface{}</code>, but told them the payload must have specific keys?</p><p>I have. And it was pretty silly. And a couple quick Google searches highlight <a href="https://medium.com/@warkiringoda/typescript-best-practices-2021-a58aee199661" target="_blank" rel="noopener noreferrer">Don't Use Any</a>. <a href="https://bitfieldconsulting.com/golang/commandments" target="_blank" rel="noopener noreferrer">Use <code>map[string]interface{}</code> sparingly</a>. Lint rules will tell you, not-so-nicely, to pound sand.</p><p>Schemas are centralized verbiage from which to generate language-specific data structures. Tools like <a href="https://github.com/quicktype/quicktype" target="_blank" rel="noopener noreferrer">Quicktype</a>, <a href="https://github.com/sinclairzx81/typebox" target="_blank" rel="noopener noreferrer">Typebox</a>, and <a href="https://www.npmjs.com/package/json-schema-to-typescript" target="_blank" rel="noopener noreferrer">jsonschema-to-typescript</a> make this a reality. The same can be said about <a href="https://www.rfc-editor.org/rfc/rfc8927" target="_blank" rel="noopener noreferrer">JTD</a> and <a href="https://developers.google.com/protocol-buffers" target="_blank" rel="noopener noreferrer">Protocol Buffers</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-power-automation">Schemas power automation<a class="hash-link" href="#schemas-power-automation" title="Direct link to heading">​</a></h3><p>Schemas make data engineering quality of life significantly better. Destination tables can be created and migrated as schemas evolve. Kafka topics and Pub/Sub streams can be automatically provisioned using the schema namespace. A single stream can be fanned out to a <a href="https://docs.aws.amazon.com/firehose/latest/dev/dynamic-partitioning.html" target="_blank" rel="noopener noreferrer">dynamically-partitioned data lake</a>. And a whole lot more.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-as-observability">Schemas as observability<a class="hash-link" href="#schemas-as-observability" title="Direct link to heading">​</a></h3><p>Calculating namespace-level statistics and splicing them into observability tools is the natural next step.</p><p>Stakeholder FAQ's (long before actual analytics) commonly look like:</p><ul><li>"I just implemeneted tracking. Is the data flowing?"</li><li>"When was some.event.v1 first deployed?"</li><li>"Is some.event.v1 still flowing?"</li><li>"Are we seeing any bad events after most recent deploy?"</li><li>"How much data are we processing for schema x.y.z?"</li><li>"I just changed some javascript. Am I still emitting one event or has it become ten?"</li><li>"What team should I go ask about a.b.c?"</li><li>"How does this event get generated again?"</li></ul><p>When the name/namespace of a schema is present with each payload, and payloads are shipped to tools like Datadog, <strong>people can self-service answers to these questions.</strong></p><p>When the name/namespace of a schema is present with each payload, and the payloads are loaded into Snowflake, <strong>people can self-service answers to these questions.</strong></p><p>Self-service, low-cognitive-load systems minimize friction and maximize efficiencies.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-power-compliance-oriented-requirements">Schemas power compliance-oriented requirements<a class="hash-link" href="#schemas-power-compliance-oriented-requirements" title="Direct link to heading">​</a></h3><p>(Actually) adhering to data privacy-oriented regulation requires a rethink of pretty much all data processing systems. The place to tokenize, redact, or hash personal information is not at the end of the data pipeline. It is at the start of the pipeline. Otherwise you'll have sensitive data lying all over S3 in cleartext or flying through Kafka with no auth, and zero clue how to actually find or mitigate it.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-are-the-foundation-of-higher-order-data-models">Schemas are the foundation of higher-order data models<a class="hash-link" href="#schemas-are-the-foundation-of-higher-order-data-models" title="Direct link to heading">​</a></h3><p>It is pretty easy to turn a schema into a <a href="https://docs.getdbt.com/docs/building-a-dbt-project/using-sources" target="_blank" rel="noopener noreferrer">dbt source</a> so analytics engineers can reliably build upon a well-defined, trustworthy foundation.</p><p>If the foundation is not strong the analytics engineering team will spend all their time building "layer 1 base models" to santize inputs. In non-professional settings this would be called <a href="https://www.youtube.com/watch?v=VoP1E9J4jpg" target="_blank" rel="noopener noreferrer">Whack A Mole</a>.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="schemas-are-the-foundation-of-data-products">Schemas are the foundation of data products<a class="hash-link" href="#schemas-are-the-foundation-of-data-products" title="Direct link to heading">​</a></h3><p>Similar to data modeling benefits, schemas allow data products to be built on a solid foundation. But there's more that can be done on top of that foundation!</p><p>Automatically-generated endpoints, GraphQL queries, and API docs? Can do. Tools like <a href="https://quicktype.io/" target="_blank" rel="noopener noreferrer">Quicktype</a>, <a href="https://transform.tools/" target="_blank" rel="noopener noreferrer">Transform</a>, and <a href="https://www.apollographql.com/" target="_blank" rel="noopener noreferrer">Apollo</a> immediately come to mind. As does a blog post from the folks at <a href="https://wundergraph.com/blog/build_json_apis_with_json_schema_by_writing_graphql_operations_against_any_datasource_like_graphql_rest_apollo_federation_postgresql_mysql" target="_blank" rel="noopener noreferrer">Wunderground</a>.</p><p>Schemas at the center is a pattern engineers are already comfortable with. OpenAPI is simply a declarative schema between API's&lt;-&gt;frontends after all.</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-contract-powered-workflow">The Contract-Powered Workflow<a class="hash-link" href="#the-contract-powered-workflow" title="Direct link to heading">​</a></h2><p>This is the workflow I've settled on after years of flipping levers and seeing what works (and what doesn't). Again, mandates don't work. Making the analytics teams happy at the expense of poor application code going into production doesn't work. Knowing instrumentation is bad only after it is deployed works-ish, but just barely. Would not recommend.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="draft-iterate-on-and-deploy-a-schema">Draft, iterate on, and deploy a schema.<a class="hash-link" href="#draft-iterate-on-and-deploy-a-schema" title="Direct link to heading">​</a></h3><p>The neat thing about schema-first workflows is <strong>even product people can write the first draft</strong>. You don't have to be an engineer to get the process going, though engineering counterparts will need to be involved eventually.</p><p>The more work that can be front-loaded to non-engineers the better. Everyone's time is valuable and schemas allow everyone to proactively contribute to the process. It sucks when useful contributions are discarded because they are "not in a usable format" (cough, Gdocs). Schemas are usable formats.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="bring-tracking-libraries-and-systems-up-to-parity">Bring tracking libraries and systems up to parity.<a class="hash-link" href="#bring-tracking-libraries-and-systems-up-to-parity" title="Direct link to heading">​</a></h3><p>The second a new schema or updated version has been published, automation kicks in and (at minimum):</p><ul><li>Builds and deploys new tracking SDK's for engineering teams</li><li>Pushes schema metadata ∆ to data discovery tools</li><li>Ensures infrastructure dependencies (Kafka topics, database tables, etc)</li><li>Pushes the schema to the appropriate place for pipeline-level validation</li><li>Creates dbt sources for the analytics engineers</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="implement-tracking">Implement tracking.<a class="hash-link" href="#implement-tracking" title="Direct link to heading">​</a></h3><p>Once systems are ready to accept the new instrumentation, engineers start implementing it into the codebase. It doesn't matter if this codebase is frontend code, backend code, cli's, infrastructure tooling, etc - the process is the same.</p><p>Getting dependencies squared away takes a matter of seconds. By the time engineers are ready to implement the new tracking, the entire system is ready to go.</p><p>A question I've heard over and over from engineers is "how do I know these payloads are making it to where they need to be?" This question is best answered with "go check Datadog/Graylog/Whatever". And followed up with, "or you could also go check Snowflake for a table of the same name".</p><p>The faster engineers have feedback the better.</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="deploy">Deploy.<a class="hash-link" href="#deploy" title="Direct link to heading">​</a></h3><p>And lastly - making tracking part of the codebase. A huge pain point of analytics-oriented instrumentation is the fact it's often identified as "bad" after being pushing to prod. This is not awesome, and it greatly contributes to the upstream "we'll just throw arbitrary json down the line" concensus. Everyone knows this is not ideal, but it's definitely better than rolling back every-other deploy due to analytics bugs.</p><p>With contract-powered workflows the following prereqs are taken care of <em>before</em> instrumentation rolls out, not after:</p><ul><li>Implementers and stakeholders talk to each other using shared verbiage.</li><li>Versioned, language-specific data structures are generated like all other code dependencies.</li><li>Metadata is pushed to discovery tools.</li><li>The pipeline is primed to accept incoming payloads, and mark them as "good" or "bad".</li><li>Observability tools are ready to go for instantaneous feedback in development and production.</li><li>Downstream analytics/modeling entrypoints (like dbt sources) are in place and can be immediately used.</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="the-schema-powered-future">The Schema-Powered Future<a class="hash-link" href="#the-schema-powered-future" title="Direct link to heading">​</a></h2><p>If it's not obvious by now, schemas are <strong>awesome</strong>.</p><p>These workflows have significantly improved my work life, I know they've improved my colleages', and it's probably just a matter of time before they improve yours too.</p><p>The fun part is it feels like this ecosystem is just getting started, and there are <strong>so</strong> many additional implications for the better. It's not a new or original concept by any means. But as data management capabililities at Non-Google companies progress, it will be a consistent solution for consistent pain.</p><p><strong>Some other reading, if you want to dive in:</strong></p><ul><li><a href="https://slack.engineering/data-wrangling-at-slack/" target="_blank" rel="noopener noreferrer">Data Wrangling at Slack</a></li><li><a href="https://www.slideshare.net/alexismidon/jitney-kafka-at-airbnb" target="_blank" rel="noopener noreferrer">Jitney at AirBnb</a></li><li><a href="https://engineering.linkedin.com/blog/2020/pegasus-data-language" target="_blank" rel="noopener noreferrer">Pegasus at LinkedIn</a></li><li><a href="https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2020/03/Schema-Integration-at-Uber-Scale-US2TS-2020-1.pdf" target="_blank" rel="noopener noreferrer">Dragon at Uber</a></li><li><a href="https://doordash.engineering/2022/08/02/building-scalable-real-time-event-processing-with-kafka-and-flink/" target="_blank" rel="noopener noreferrer">Building Scalable Real Time Event Processing with Kafka and Flink</a> at Doordash</li><li><a href="https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873" target="_blank" rel="noopener noreferrer">Data Mesh at Netflix</a></li><li><a href="https://shopify.engineering/real-time-buyer-signal-data-pipeline-shopify-inbox" target="_blank" rel="noopener noreferrer">Building a Real-time Buyer Signal Data Pipeline for Shopify Inbox</a></li></ul><p><strong>You could also:</strong></p><ul><li>Tune in for <a href="https://coalesce.getdbt.com/agenda/getting-jiggy-with-jsonschema-the-power-of-contracts-for-building-data-systems" target="_blank" rel="noopener noreferrer">Getting jiggy with JSON Schema at dbt Coalesce</a> or <a href="https://twitter.com/emilyhawkins__" target="_blank" rel="noopener noreferrer">join the conversation</a> on <a href="https://twitter.com/aerialfly" target="_blank" rel="noopener noreferrer">Twitter</a>.</li><li>Say hi in <a href="https://locallyoptimistic.slack.com/archives/C043RDEFMBL" target="_blank" rel="noopener noreferrer">Locally Optimistic Slack</a>.</li></ul><p>As closing context from a Shopify perspective - 9800+ schemas and 1800+ contributors (many of whom are not engineers) is a pretty big feat. As is deploying hundreds of schema-generated instrumentation to thousands of robots around the world. The model works. Here's to the schema-powered future 🥂.</p>]]></content:encoded>
            <category>data platform</category>
            <category>schemas</category>
            <category>interfaces</category>
            <category>data contracts or whatever</category>
        </item>
        <item>
            <title><![CDATA[Make It Easy]]></title>
            <link>https://buz.dev/blog/make-it-easy</link>
            <guid>make-it-easy</guid>
            <pubDate>Thu, 06 Oct 2022 19:36:03 GMT</pubDate>
            <description><![CDATA[Defining, implementing, and evolving events is hard.]]></description>
            <content:encoded><![CDATA[<p>Defining, implementing, and evolving events is hard.</p><p>Running event collection systems is also hard.</p><p>Buz aims to make it easy.</p><p><strong><a href="/">What is Buz?</a></strong></p>]]></content:encoded>
        </item>
    </channel>
</rss>