"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[1477],{10:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"make-it-easy","metadata":{"permalink":"/blog/make-it-easy","source":"@site/blog/oh-hello/index.md","title":"Make It Easy","description":"Defining, implementing, and evolving events is hard.","date":"2023-02-22T19:10:24.000Z","formattedDate":"February 22, 2023","tags":[],"readingTime":0.115,"hasTruncateMarker":false,"authors":[{"name":"Jake","title":"\ud83d\udc1d","url":"https://github.com/jakthom","imageURL":"https://github.com/jakthom.png","key":"jake"}],"frontMatter":{"slug":"make-it-easy","title":"Make It Easy","authors":["jake"],"tags":[]},"nextItem":{"title":"Serverless Makes Streaming Accessible","permalink":"/blog/serverless-makes-streaming-accessible"}},"content":"Defining, implementing, and evolving events is hard.\\n\\nRunning event collection systems is also hard.\\n\\nBuz aims to make it easy.\\n\\n\\n**[What is Buz?](/)**"},{"id":"/serverless-makes-streaming-accessible","metadata":{"permalink":"/blog/serverless-makes-streaming-accessible","source":"@site/blog/serverless-makes-streaming-accessible/index.md","title":"Serverless Makes Streaming Accessible","description":"Snowplow Analytics is a highly-scalable system that empowers structured data creation for millions of sites on the internet. Snowplow tracking is incorporated into dbt, dbt cloud, Trello, Gitlab, Citi bank, Backcountry.com, and the list goes on.","date":"2023-02-22T00:00:00.000Z","formattedDate":"February 22, 2023","tags":[{"label":"Serverless Snowplow Analytics","permalink":"/blog/tags/serverless-snowplow-analytics"},{"label":"Serverless Event Tracking","permalink":"/blog/tags/serverless-event-tracking"},{"label":"Google Cloud Run","permalink":"/blog/tags/google-cloud-run"},{"label":"Pub/Sub","permalink":"/blog/tags/pub-sub"},{"label":"BigQuery","permalink":"/blog/tags/big-query"},{"label":"Snowflake","permalink":"/blog/tags/snowflake"},{"label":"Postgres","permalink":"/blog/tags/postgres"}],"readingTime":11.6,"hasTruncateMarker":true,"authors":[{"name":"Jake","title":"\ud83d\udc1d","url":"https://github.com/jakthom","imageURL":"https://github.com/jakthom.png","key":"jake"}],"frontMatter":{"title":"Serverless Makes Streaming Accessible","authors":["jake"],"slug":"/serverless-makes-streaming-accessible","tags":["Serverless Snowplow Analytics","Serverless Event Tracking","Google Cloud Run","Pub/Sub","BigQuery","Snowflake","Postgres"],"date":"2023-02-22T00:00","hide_table_of_contents":false},"prevItem":{"title":"Make It Easy","permalink":"/blog/make-it-easy"},"nextItem":{"title":"The Contract-Powered Data Platform","permalink":"/blog/the-contract-powered-data-platform"}},"content":"[Snowplow Analytics](https://snowplow.io/) is a highly-scalable system that empowers [structured data creation](https://snowplow.io/blog/why-data-contracts-are-obviously-a-good-idea/) for [millions of sites](https://trends.builtwith.com/analytics/Snowplow) on the internet. Snowplow tracking is incorporated into [dbt](https://github.com/dbt-labs/dbt-core/blob/main/core/dbt/tracking.py#L33-L47), [dbt cloud](https://cloud.getdbt.com/), [Trello](https://trello.com/), [Gitlab](https://gitlab.com/), [Citi bank](https://www.citi.com/), [Backcountry.com](https://www.backcountry.com/), and the list goes on.\\n\\nAfter setting up data infrastructure [like Snowplow](https://bostata.com/268-billion-events-with-snowplow-snowflake-at-cargurus) for [years](https://bostata.com/client-side-instrumentation-for-under-one-dollar)  I\'ve frequently found myself wishing for both **less** and **more**.\\n\\n**Fewer streams**, **fewer machines or containers to manage**, **fewer moving pieces to help prevent event duplication or loss**, **less configuration**, and **less in-house documentation to keep things running** would be a dream.\\n\\n**Deployment flexibility**, **flexible schema storage**, **cost efficiencies**, **seamless migration between transport systems**, **improved utility from the data in transit**, and **increased visibility** would also be very helpful.\\n\\n\\nMeanwhile, serverless technologies have come into their own and point the way toward a very bright data-processing future. Which is how **[buz.dev](https://buz.dev)** was born.\\n\\n\x3c!-- truncate --\x3e\\n\\n\\n## Inspirations and iterations\\n\\nMy first iteration of \\"serverless Snowplow Analytics\\" was in 2018.  I stitched together a [Cloudfront distribution](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-working-with.html), a [Python-based Lambda function](https://docs.aws.amazon.com/lambda/latest/dg/lambda-python.html), S3, and the [Snowplow javascript tracker](https://www.jsdelivr.com/package/npm/@snowplow/javascript-tracker?path=dist) to stockpile event data. A series of [Athena external tables](https://docs.aws.amazon.com/athena/latest/ug/creating-tables.html) sat on top of the raw data and voila! Near-real-time analytics for practically free. It worked so well [a blog post was written](https://bostata.com/client-side-instrumentation-for-under-one-dollar/) and other people were inspired to [build](https://discourse.snowplow.io/t/snowplow-serverless/1912/14) or [write about](https://www.ownyourbusinessdata.net/enrich-snowplow-data-with-aws-lambda-function/) the same. The system ran hands-off with very little effort on my part, thanks to **minimal moving pieces** and **AWS having the responsibility of keeping it up and running**. \\n\\nServerless event collection worked very well at scale during my time at [CarGurus](https://www.cargurus.com/). While we had a Snowplow Analytics implementation, a colleage wanted to see what AWS Lambda could handle. The marketing team sent enormous amounts of email using a combination of [Iterable](https://iterable.com/) and [Dyn](https://help.dyn.com/email-delivery-gsg/), and each blast would result in production systems being absolutely swamped with tracking callbacks. The analytics team wanted data for analytics or tracking opt-outs, but nobody wanted to provision (normally unused) static infrastructure. Lambda was a perfect fit. The system continuously ramped from 0 to ~20k rps and back again, the AWS bill was laughably small, and it required virtually zero maintenance.\\n\\nAfter setting up Snowplow for a [NYC-based commercial real estate company](https://www.bisnow.com/), the VP of Technology (who is now @ Disney Streaming) pushed my implementation further with a simple Lambda function. Instead of being limited to Snowplow protocol payloads, the Lambda function collected arbitrary json payloads, reformatted them as Snowplow [Self-Describing Events](https://docs.snowplow.io/docs/understanding-tracking-design/out-of-the-box-vs-custom-events-and-entities/#self-describing-events), and fired them into the Snowplow collector. It was efficient and effectively hands-off. I have thought about his work ever since.\\n\\nThe rich benefits of serverless data processing for robotics and fulfillment were experienced while working at [6 River Systems](https://6river.com/data-driven-robotics-leveraging-google-cloud-platform-and-big-data-to-improve-robot-behaviors/) (Shopify Logistics). Data volumes from fulfillment systems are highly variable. One warehouse or distribution center will have a very different traffic pattern than another, but volume across all of them spikes to orders of magnitudes higher [during Peak months](https://supplychaingamechanger.com/strategies-to-survive-the-peak-season-fulfillment-surge/). Over-provisioned infrastructure in an industry where margins are already tight is a complete non-starter. Serverless was the only option and did not disappoint.\\n\\nAnd finally, my time working alongside Okta\'s security team has thoroughly solidified the value of serverless technologies in a security-conscious setting. _Security teams absolutely love serverless tech._ Want proof? Go check out [Matano.dev](https://www.matano.dev/), ask [Panther](https://panther.com/) how their systems are built, or dig into [AWS marketing materials](https://aws.amazon.com/blogs/publicsector/how-public-sector-security-teams-can-use-serverless-technologies-improve-outcomes/).\\n\\n\\n**Serverless is the secret of highly efficient data processing. It is the way to make streaming accessible.**\\n\\n\\n# Why Build Buz?\\n\\n\\nWhile incredibly scalable and robust, setting up and maintaining OSS Snowplow is not for the faint of heart. It\'s time-consuming to set up, requires a deep understanding of the moving pieces to tune well, and requires significant engineering resources.\\n\\nA very common Snowplow architecture diagram looks like the following, excluding monitoring, alerting, log centralization, and other devops necessities:\\n\\n![snowplow](img/snowplow_arch.png)\\n\\n\\nOpportunity costs matter to cost-conscious buinesses, and engineering resources dedicated to maintaining data pipelines are rarely the best use of said resources. ***Engineers also prefer to spend their time using streaming data rather than moving it around***.\\n\\n\\nSo my initial goal was to build a system like the following:\\n\\n![serverless thing](img/initial_arch.png)\\n\\nSnowplow tracking SDK\'s would be used for instrumentation. The serverless collector thing called Buz would collect, validate, and route payloads to S3 via Kinesis Firehose. And Snowflake would provide the compute on top of S3.\\n\\n\\n## Buz Requirements and Design\\n\\n\\n### Minimal human involvement to keep running\\n\\nSystems are great when you don\'t need to think about them. In [Julia Evans\'](https://twitter.com/b0rk) words, spending **[approximately 0 time on operations](https://jvns.ca/blog/2022/07/09/monitoring-small-web-services/)** was the goal.\\n\\n\\n### Self-contained and capable of running horizontally with no issue\\n\\nThere\'s a movement of \\"small, mighty, and self-contained\\" afoot within data processing systems.\\n\\n*It\'s because complexity is hard to keep running*.\\n\\nSystems like [Redpanda](https://redpanda.com/), which crams the Kafka api into a small self-contained binary, or [Benthos](https://www.benthos.dev/), which crams cool stream-processing functionality into a small self-contained binary, are highly inspirational.\\n\\nThe Serverless Thing called Buz needed to do the same.\\n\\n### No JVM, no Spark, no Beam\\n\\nSnowplow\'s [collector](https://docs.snowplow.io/docs/pipeline-components-and-applications/stream-collector/), [enricher](https://docs.snowplow.io/docs/pipeline-components-and-applications/enrichment-components/enrich/#enrich-kinesis), [s3 sink](https://docs.snowplow.io/docs/pipeline-components-and-applications/loaders-storage-targets/s3-loader/), etc all run on the JVM.\\n\\nSnowplow\'s [RDB](https://docs.snowplow.io/docs/pipeline-components-and-applications/loaders-storage-targets/snowplow-rdb-loader-3-0-0/) and [Snowflake](https://docs.snowplow.io/docs/pipeline-components-and-applications/loaders-storage-targets/snowplow-snowflake-loader/) loaders run on Spark while the [BigQuery](https://docs.snowplow.io/docs/pipeline-components-and-applications/loaders-storage-targets/bigquery-loader/) loader runs on Beam (Cloud Dataflow).\\n\\nBut... Snowflake\'s [Snowpipe](https://docs.snowflake.com/en/user-guide/data-load-snowpipe-intro.html) works well, as do BigQuery [streaming inserts](https://cloud.google.com/bigquery/docs/samples/bigquery-table-insert-rows) or  [Pub/Sub subscriptions](https://cloud.google.com/pubsub/docs/bigquery). And the responsibility of keeping Snowpipe or BQ streaming inserts running is offloaded :).\\n\\n\\nServerless Thing was to shed as many dependencies as possible.\\n\\n### Fast startup and shutdown\\n\\nMaking containers fast to launch makes a big impact on cost as invocations ramp, so Serverless Thing had to be snappy. The faster infrastructure can follow the utilization curve, the more cost-effective it is. In an environment where costs are being scrutinized, **doing work fast** is just as important as **not running at all when there is no work to be done**.\\n\\nBeing efficient also happens to be pretty damn good for the environment. Burning fewer polar bears seems to [resonate with others](https://medium.com/@intive/this-months-reason-technology-will-save-the-world-energy-savings-and-serverless-principles-375660c8ed81) like [DuckDB](https://youtu.be/Z-6SnP6yzgo?t=1826) and [451 Research](https://d39w7f4ix9f5s9.cloudfront.net/e3/79/42bf75c94c279c67d777f002051f/carbon-reduction-opportunity-of-moving-to-aws.pdf). \\n\\n### Payload validation, annotation, and bifurcation\\n\\nA very valuable Snowplow feature lies at the `Enricher`, where each and every event is validated using the associated jsonschema.\\n\\nThe only way to do this quickly is via a self-warming schema cache, so an onboard cache became another requirement.\\n\\n### Just JSON\\n\\nSnowplow data is serialized using **[thrift](https://thrift.apache.org/)** between the collector and the enricher but becomes **tsv** downstream of the enricher. This makes it hard to point a system like [Materialize](https://materialize.com/) at the \\"enriched\\" stream without first `reading tsv records` -> `formatting as json` -> `writing to a separate stream`. Write amplification quickly becomes reality and the operator must make a choice between **not reading from the stream** or **re-formatting every payload to something that is easily pluggable with other stream processing systems**. At higher volumes this equates to $$$$$.\\n\\nWhile JSON is not the smallest data format it is still more efficient to write JSON once than having many copies of smaller formats. I chose to have **fewer** copies but a **larger per-record format**.\\n\\nThis decision is tbd. In the worst case it\'s easy to change to parquet depending on destination.\\n\\n### Easy to configure\\n\\nYaml + Jsonschema validation is [becoming pretty standard](https://www.schemastore.org/json/). It turned out to be a pretty good decision since auto-completing, auto-validating config is handy.\\n\\nServerless Thing had to be easy to configure. Bonus points for providing hints in an editor throughout the configuration process.\\n\\n### Make event streaming accessible\\n\\n[dbt](https://www.getdbt.com/) has been so inspirational because it makes good data engineering practices accessible to all. Tricks that used to pay rent have become dbt packages anyone can import.\\n\\nLike the data engineering of not-that-long-ago, today\'s streaming systems are **intimidating**. But they don\'t need to be. Streaming systems are also often **overkill**. Throwing data into several streams only to batch-insert it into a Postgres database means the streaming infrastructure is probably unnecessary.\\n\\nIdeally Serverless Thing could make streaming accessible while empowering orgs to evolve from the current stack to some desired architecture. Event if that means shipping events to [Postgres](https://buz.dev/integrations/postgres) now and [Kafka](https://buz.dev/integrations/kafka) later.\\n\\n\\n# Progress thus far\\n\\nWhile Buz has much further to go, the journey of serverless event tracking has already been extremely worthwhile.\\n\\nThe pain and complexity of streaming systems seems to resonate with ***many*** people. **Serverless fixes this pain.**\\n\\n### Expanding to more inputs\\n\\nEarly on in the exploration I had a eureka moment - if the serverless model works using [Snowplow\'s tracker protocol](https://docs.snowplow.io/docs/collecting-data/collecting-from-own-applications/snowplow-tracker-protocol/) it should work for other protocols. As it turns out, it does! While also minimizing the hassle of running multiple event tracking pipelines - such as one pipeline for each protocol.\\n\\n[Cloudevents](/inputs/cloudNative/cloudevents) with its (optional) [dataschema](https://cloudevents.github.io/sdk-javascript/interfaces/event_interfaces.CloudEventV1.html#dataschema) property was a low-effort addition. Fire payloads using the Cloudevents\' `data` property, provide a schema reference in `dataschema`, and voila! Validated events without needing to [write the sdk](https://github.com/cloudevents?q=sdk-&type=all&language=&sort=), or quickly hooking into tracking already in existence.\\n\\n\\nData collection using pixels and webhooks was a fun addition, mostly because both of these sources are often painful due to the arbitrary nature of their payloads. But another thought came to mind - validate these too! Since it would be fab to namespace and validate these payloads, named [pixels](/inputs/buz/pixel#named-pixels) and [webhooks](/inputs/buz/webhook#named-webhooks) came to be.\\n\\nAccepting [self-describing](/inputs/buz/self-describing) payloads was a nice addition, and provides some additional flexibility like custom top-level payload property names. It also makes internal SDK\'s a breeze to build.\\n\\nIn past work lives I\'ve built lightweight sidecars to read [Mongodb change streams](https://www.mongodb.com/docs/manual/changeStreams/) or [Postgres logical replication](https://www.postgresql.org/docs/current/logical-replication.html) before writing data to systems like Kafka. Serverless Thing naturally lends itself to supporting change data capture, or at least the weird cousin of what CDC looks like today.\\n\\nA lovely example of what could (and/or should) be ubiquitous is [Devoted Health\'s Avalanche](https://tech.devoted.com/avalanche-streaming-postgres-to-snowflake-130e8c477f07?gi=db8239b2a6ad) project. \\n\\n### Writing events to a variety of destinations\\n\\n`Kinesis`, `Kafka`, and `Pub/Sub` are commonly-supported streaming/transport mechanisms for data systems. All three options work well. But they are often **overkill and debatably net-negative** for:\\n\\n- Companies with low data volumes or [not big data](https://motherduck.com/blog/big-data-is-dead/)\\n- Companies that don\'t [need or want legitimately-real-time data access](https://twitter.com/tayloramurphy/status/1625995802924445697)\\n- Non-production environments\\n- Local development\\n\\nAt CarGurus I learned the importance of making systems accessible to engineers, regardless of which environment they run in. Product engineers care about one thing: **data gets to where it needs to be, wherever that is.** They typically do not care about **how it gets there** or **where it needs to be for downstream consumption**.\\n\\nAt Shopify I learned the intricacies of building systems that can be deployed in single-tenant fashion as efficiently as multi-tenant.\\n\\nHaving flexibility to write data to a variety of systems is a requirement for all of the above, so Buz quickly expanded to support:\\n\\n- **[Streaming destinations](/category/streaming-sinks)** which are best in production at scale.\\n- **[Streaming hybrids like Kinesis Firehose](/outputs/stream/kinesis-firehose)**, because they are hands-off and incredibly powerful for building data lakes.\\n- **[Traditional databases](/category/database-sinks)**, because most every company has one already.\\n- **[Streaming databases](/outputs/database/materialize)**, because they are the future.\\n- **[Timeseries databases](/category/timeseries-sinks)**, because they unlock some very interesting use cases.\\n- **[Saas products](/category/saas-sinks)**, because product and engineering teams everywhere use them.\\n- **[Message brokers](/category/message-broker-sinks)**, because they are well-loved and very useful.\\n\\n### Writing events to >1 destination at the same time\\n\\nShopify has a streaming model where **events are written to both Datadog for observability and Kafka for distribution to the data lake.** A secondary model is **simultaneously writing product/marketing events to Amplitude for product analytics and Kafka for distribution to the data lake.** After seeing how simple yet operationally powerful these are, I had a hard time ignoring them.\\n\\nMigrating systems is a very common pain point as needs, volume, and organizations evolve. Migrating from `Postgres` to `Kafka` or `Kinesis` to `Kafka` are common patterns, and dual writing is the way to do this without blowing everything up. I\'ve often wanted to simply add a configuration block instead of writing a new system that will be thrown away after migration.\\n\\n**So Buz supports writing to more than one destination. **\\n\\nThere are tradeoffs here that must not be ignored, as the risk of one destination being unavailable goes up pretty quickly when the number of them increases. It\'s a worthwhile ops lever nonetheless.\\n\\n### Flexible schema registry backends (including using the destination system as a registry)\\n\\nI\'ve often questioned why data processing systems rarely use the backend they ship data to for serving configuration, schemas, and other runtime resources. Many web apps do this - why don\'t data systems?\\n\\nIn the spirit of minimizing moving pieces, and because it\'s fun, Buz supports a variety of schema registry backends.\\n\\nInteresting use cases this functionality unlocks include:\\n\\n- **Streaming analytics with no streams.** Using [Materialize](/outputs/database/materialize) as the destination as well as the [schema registry](under-the-hood/registry/backends/database/materialize) means streaming insights don\'t rely on much infrastructure.\\n- **Using Postgres as the application database, the event database, and the schema cache.** Introducing event tracking to existing systems has literally never been easier.\\n- **Analytics without a database at all.** Using GCS or S3 for the schema cache and the data lake means a database is not required to get database-like results.\\n- **Seamless tie-in to existing streaming workflows.** If a Kafka or Redpanda schema is already in place, perfect! Yet-another-piece-of-infrastructure\u2122 should not be necessary.\\n\\nThese unlocks are incredibly exciting.\\n\\n\\n# Where do we go from here?\\n\\nThanks to serverless tech, we are in the early innings of complete data infrastructure transformation.\\n\\nThe serverless-first data processing idea seemed crazy for a very long time, **but it\'s definitely not crazy.** Companies like [Modal](https://modal.com/) and [Panther](https://www.snowflake.com/powered-by/panther-labs/) have been built from the ground-up to power data-oriented serverless workloads, [Fivetran](https://www.fivetran.com/blog/serverless-etl-with-cloud-functions) leverages serverless for [custom connectors](https://fivetran.com/docs/functions), DuckDB can be easily [tossed into a serverless function](https://twitter.com/mim_djo), database drivers are [being retooled](https://planetscale.com/blog/introducing-the-planetscale-serverless-driver-for-javascript) for [serverless workloads](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-serverless-v2.html), and the list goes on.\\n\\nServerless enables highly-efficient, secure, and low-footprint data workloads. It drastically lowers the complexity bar of data processing systems, and enables teams to spend less time on the boring stuff.\\n\\nBuz will continue to be all-in on serverless because it makes streaming accessible."},{"id":"the-contract-powered-data-platform","metadata":{"permalink":"/blog/the-contract-powered-data-platform","source":"@site/blog/contract-powered-platform/index.md","title":"The Contract-Powered Data Platform","description":"The contract-powered data platform is a step towards improving data quality, reducing organizational friction, and automating the toil data teams face. Here\'s what it looks like and how it works.","date":"2022-10-06T00:00:00.000Z","formattedDate":"October 6, 2022","tags":[{"label":"data platform","permalink":"/blog/tags/data-platform"},{"label":"schemas","permalink":"/blog/tags/schemas"},{"label":"interfaces","permalink":"/blog/tags/interfaces"},{"label":"data contracts or whatever","permalink":"/blog/tags/data-contracts-or-whatever"}],"readingTime":12.955,"hasTruncateMarker":true,"authors":[{"name":"Jake","title":"\ud83d\udc1d","url":"https://github.com/jakthom","imageURL":"https://github.com/jakthom.png","key":"jake"}],"frontMatter":{"slug":"the-contract-powered-data-platform","title":"The Contract-Powered Data Platform","description":"The contract-powered data platform is a step towards improving data quality, reducing organizational friction, and automating the toil data teams face. Here\'s what it looks like and how it works.","authors":["jake"],"tags":["data platform","schemas","interfaces","data contracts or whatever"],"date":"2022-10-06T00:00"},"prevItem":{"title":"Serverless Makes Streaming Accessible","permalink":"/blog/serverless-makes-streaming-accessible"}},"content":"Between [6 River Systems](https://6river.com/?utm_source=buz.dev&utm_content=hiitsme) and [CarGurus](https://www.cargurus.com/), a very significant amount of my time over the past five years has been dedicated to data platform automation, reducing cross-team friction, and improving data quality.\\n\\nSchemas have played a critical role in the process; this post outlines **the why and the how**. But before diving straight\x3c!--truncate--\x3e into the role of schemas (er... contracts) let\'s talk data platforms.\\n\\n\\n\\n\\n## Components of a Good Data Platform\\n\\n### Instrumentation and Integration\\nThis one goes without saying. If data is not being emitted from source systems you won\'t have any data to play with.\\n\\nIf you don\'t have any data the rest of this post will not provide value. You also won\'t be able to complain about the price of Snowflake and might feel left out. \\n\\nInstrumentation is pretty important. It\'s also a pretty huge PITA to wrangle, which is why [tracking](https://segment.com/academy/collecting-data/how-to-create-a-tracking-plan/) [plans](https://amplitude.com/blog/create-tracking-plan) [became](https://www.avo.app/blog/what-is-a-tracking-plan-and-why-do-you-need-one) [a](https://www.indicative.com/resource/data-tracking-plan/) [thing](https://www.trackingplan.com/).\\n\\n### The pipeline\\n\\nPipelines are either `batch` or `streaming`. There\'s a holy war between the two religions, but similar concepts apply to both.\\n\\nPipelines collect data and put it somewhere. Sometimes they mutate said data. That\'s really it.\\n\\nThe _best_ pipelines:\\n\\n- Move data reliably.\\n- Annotate payloads with metadata such as provenance, `collected_at` timestamps, fingerprints, etc.\\n- Generate stats to provide the operator with feedback.\\n- Validate and bifurcate payloads, if you\'re lucky.\\n- Know about and act on payload sensitivities - obfuscate, hash, tokenize, redact, redirect, etc.\\n- Minimize moving pieces.\\n- Don\'t spend all the CEO\'s \ud83d\udcb8\ud83d\udcb8\ud83d\udcb8 so they can afford that house in the Bahamas.\\n\\n\\n### Storage and access\\n\\nData has to be stored somewhere- preferably somewhere accessible.\\n\\nStorage/access systems range from a wee little Postgres database, to Snowflake, to a data lake filled with Parquet fish and the ~~Loch Ness~~ Trino monster.\\n\\n### Data Discovery\\n\\nAs things scale, pipelines/databases/data models often turn into something the James Webb *and* dbt can\'t stitch back together.\\n\\nData discoverability is super important when organizations are fragmented, or when you\'re new to the company, or when you forget stuff as I often do.\\n\\n\\n### Observability, Monitoring, and Alerting\\n\\nLast, definitely not least, and unfortunately-rare... tools that tell the operator if things are broken.\\n\\nThese could be devops-y tools like Prometheus/Alertmanager/Grafana, pay-to-play tools like data quality/reliability platforms, or something dead-simple like load metadata tables and freshness checks.\\n\\n\\n## Design Goals of a Good Data Platform\\n\\n### Comply with rules\\n\\nWhile maybe not the case within the US (for us Non-Californians), data regulation and compliance is kind of a big deal. If you don\'t comply, [good](https://www.wired.com/story/google-analytics-europe-austria-privacy-shield/) [luck](https://edpb.europa.eu/news/national-news/2022/italian-sa-bans-use-google-analytics-no-adequate-safeguards-data-transfers_en).\\n\\nCompliance is becoming less of a `goal` and more of a `requirement`.\\n\\n### Minimize bad data\\n\\nBad data is expensive. It\'s expensive to move, it\'s expensive to store, it\'s expensive to keep track of, and it\'s expensive to work around. Not knowing data is bad is even more expensive.\\n\\n### Maximize knowledge of what the system is doing\\n\\nGood things come from a knowledge of what a system is doing and when it is doing it.\\n\\nOnly after measurement can you optimize cost.\\n\\nOnly after timing can you make things faster.\\n\\nAnd only after seeing a system end-to-end can you cut out unnecessary intermediaries.\\n\\n\\n### Minimize friction for all parties involved\\n\\nData platforms should be a good experience for everyone. Which includes **many** more parties than just analytics engineers or analysts.\\n\\nParties who are critical to success include:\\n\\n- The frontend engineers, who work with Javascript/Typescript and any number of frameworks \\n- The backend engineers, who work with Python, Node, Java, Go, C++\\n- The native/app engineers, who work with something like Swift, Flutter, React Native\\n- The devops people, who like when they can write less Terraform.\\n- The SRE people, who like when they can see what\'s going on without asking you. Because you\'ll probably be asleep when they try.\\n- The data engineers, who are usually on the hook when data goes bad.\\n- The analytics engineers, who like when `user_id` means `user_id`.\\n- The analysts, who like when they can push value back to product engineers.\\n- The financefolk, who will come after you when costs exponentially increase.\\n- The businessfolk, who will also come after you when costs exponentially increase.\\n\\nWhile data mandates and a new breed of data-oriented law might sound lovely (or not), these mechanisms only benefit a couple of the above parties. Mandates don\'t work. Telling other people to have more responsibility, so you can have less, also doesn\'t work.\\n\\nWant to get buy-in? Minimize friction. Want to increase adoption? Automate others\' toil. Want sustainable systems? Reduce cognitive load.\\n\\nWhich brings us back to schematizing stuff.\\n\\n## The Contract-Powered Platform\\n\\nI\'m going to go out on a limb and just say it -> schemas are the nucleus of sustainable data platforms.\\n\\nSchematizing data upfront is often initially discarded and seen as [unnecessary overhead](https://twitter.com/Mike_Kaminsky/status/1573430588958445569) or a productivity drain. The idea is nixed in favor of the eventual chaos arbitrary json creates. But who cares about our hypothetical future selves - it\'s our current selves that matter. Let\'s dig in for the sake of science.\\n\\n\\n### Schemas empower the \\"producer\\" <-> \\"consumer\\" relationship\\n\\nLet\'s think about the two ends of data systems for a second.\\n\\nEngineers exist at the source, product analysts typically exist at the \\"destination\\", and a black box lies between:\\n\\n![mystery data thing](img/mystery_data_thing.png)\\n\\n\\nBut when the mystery data thing is removed, the `engineer` <-> `analyst` dynamic actually looks more like this:\\n\\n![engineers and analysts](img/engs_and_analysts.png)\\n\\n\\nThis working dynamic is pretty terrible for productivity. The two parties communicate, sometimes. There\'s a ton of friction and neither party is to blame. Data engineers and managers are asked to join the conversation, and implicit \\"contracts\\" are established in a Google doc that everyone will lose track of.\\n\\n(Human-readable) schemas turn this dynamic into something that looks more like the following:\\n\\n![declarative thing](img/declarativeThing.png)\\n\\nBoth parties contribute to a schema with consistent verbiage, which is then leveraged to generate the equivalent data structure in their language of choice.\\n\\nToday\'s data workflows look **very similar** to how software engineering looked prior to [Github announcing Pull Requests in 2008](https://github.blog/2008-02-23-oh-yeah-there-s-pull-requests-now/). They work, but aren\'t ideal.\\n\\n\\n### Schemas are data discovery\\n\\nIn LinkedIn\'s [popular metadata architectures explained](https://engineering.linkedin.com/blog/2020/datahub-popular-metadata-architectures-explained), pull-based metadata extraction is outlined as \\"an ok start\\". Push-based metadata is \\"ideal\\".\\n\\nSchematizing data upfront means data discovery and documentation writes itself. Data assets are discoverable as soon as schemas are deployed - before the data actually starts flowing.\\n\\nSchematization mechanisms like JSON Schema also get [pretty meta](https://json-schema.org/specification.html#meta-schemas), so it\'s easy to add annotation such as:\\n\\n- \\"These properties contain PII\\"\\n- \\"These properties should be tokenized\\"\\n- \\"X person on Y team owns this schema\\"\\n- \\"This is version `1.4`. Here\'s how this data has evolved from `1.0`.\\"\\n\\nThis class of metadata is a CISO\'s _dream_.\\n\\n### Schemas power data validation in transit\\n\\nComparing a payload to \\"what it was supposed to be\\" and annotating it with a simple \ud83d\udc4d\ud83d\udc4e is extremely valuable. Schemas are the \\"what it was supposed to be\\".\\n\\n### Schemas help stop bad instrumentation from being implemented in the first place\\n\\nAnother +1 (for engineers) is the fact schemas help prevent bad tracking from getting deployed in the first place.\\n\\nLanguage-specific data structures can be generated from schemas. Which means intellisense or the compiler complains during development if required props are missing, or if one is a `string` and should be a `bool`. And then the code blows up again at compile time if the bug is still there.\\n\\nNobody likes being the person who causes the release train to halt. Or being the person who caused the rollback because a prop was missing. **Especially when it\'s \\"just for analytics\\".**\\n\\nMerging to `main` only after instrumentation is \ud83d\udc4d\ud83d\udc4d is the ideal workflow. It saves rollbacks. It avoids the human friction of going to the data team... *again*, to have them explain their mandated \\"contract\\".\\n\\nAnd it\'s just good engineering.\\n\\n\\n### Schemas improve code quality\\nThis might be a stretch. Or maybe not.\\n\\nHave you tried forcing an engineer who loves Typescript to use `Any`, while simultaneously mandating payloads have `propA`, `propB`, and `propC`. And `propC` must be a `bool`?\\n\\nOr tried forcing a golang-oriented engineer to use a `map[string]interface{}`, but told them the payload must have specific keys?\\n\\nI have. And it was pretty silly. And a couple quick Google searches highlight [Don\'t Use Any](https://medium.com/@warkiringoda/typescript-best-practices-2021-a58aee199661). [Use `map[string]interface{}` sparingly](https://bitfieldconsulting.com/golang/commandments). Lint rules will not-so-nicely tell you to pound sand.\\n\\nSchemas are centralized verbiage from which to generate language-specific data structures. Tools like [Quicktype](https://github.com/quicktype/quicktype), [Typebox](https://github.com/sinclairzx81/typebox), and [jsonschema-to-typescript](https://www.npmjs.com/package/json-schema-to-typescript) make this a reality. The same can be said about [JTD](https://www.rfc-editor.org/rfc/rfc8927) and [Protocol Buffers](https://developers.google.com/protocol-buffers).\\n\\n\\n### Schemas power automation\\n\\nSchemas make data engineering quality of life significantly better. Destination tables can be automatically created and migrated as schemas evolve. Kafka topics and Pub/Sub streams can be automatically provisioned using the schema namespace. A single stream can be fanned out to a [dynamically-partitioned data lake](https://docs.aws.amazon.com/firehose/latest/dev/dynamic-partitioning.html). And a whole lot more.\\n\\n\\n### Schemas as observability\\n\\nCalculating namespace-level statistics and splicing them into observability tools is the natural next step.\\n\\nStakeholder FAQ\'s (long before actual analytics) commonly look like:\\n- \\"I just implemented tracking. Is the data flowing?\\"\\n- \\"When was some.event.v1 first deployed?\\"\\n- \\"Is some.event.v1 still flowing?\\"\\n- \\"Are we seeing any bad events after most recent deploy?\\"\\n- \\"How much data are we processing for schema x.y.z?\\"\\n- \\"I just changed some javascript. Am I still emitting one event or has it become ten?\\"\\n- \\"What team should I go ask about a.b.c?\\"\\n- \\"How does this event get generated again?\\"\\n\\n\\nWhen the name/namespace of a schema is present with each payload, and payloads are shipped to tools like Datadog, **people can self-service answers to these questions.**\\n\\nWhen the name/namespace of a schema is present with each payload, and the payloads are loaded into Snowflake, **people can self-service answers to these questions.**\\n\\nSelf-service, low-cognitive-load systems minimize friction for everyone.\\n\\n### Schemas power compliance-oriented requirements\\n\\n(Actually) adhering to data privacy-oriented regulation requires a rethink of pretty much all data processing systems. The place to tokenize, redact, or hash personal information is not at the end of the data pipeline. It is at the start. Otherwise you\'ll have sensitive data lying all over S3 in cleartext or flying through Kafka with no auth, and zero clue how to actually find or mitigate it.\\n\\n\\n### Schemas are the foundation of higher-order data models\\n\\nIt is pretty easy to turn a schema into a [dbt source](https://docs.getdbt.com/docs/building-a-dbt-project/using-sources) so analytics engineers can reliably build upon a well-defined, trustworthy foundation.\\n\\nIf the foundation is not strong the analytics engineering team will spend all their time building \\"layer 1 base models\\" to santize inputs. In non-professional settings this would be called [Whack A Mole](https://www.youtube.com/watch?v=VoP1E9J4jpg).\\n\\n\\n### Schemas are the foundation of data products\\n\\nSimilar to data modeling benefits, schemas allow data products to be built on a solid foundation. But there\'s more that can be done on top of that foundation!\\n\\nAutomatically-generated endpoints, GraphQL queries, and API docs? Can do. Tools like [Quicktype](https://quicktype.io/), [Transform](https://transform.tools/), and [Apollo](https://www.apollographql.com/) immediately come to mind. As does a blog post from the folks at [Wundergraph](https://wundergraph.com/blog/build_json_apis_with_json_schema_by_writing_graphql_operations_against_any_datasource_like_graphql_rest_apollo_federation_postgresql_mysql).\\n\\n\\"Schemas at the center\\" is a pattern engineers are already comfortable with. [OpenAPI](https://www.openapis.org/) is simply a declarative schema between API\'s<->frontends after all.\\n\\n\\n## The Contract-Powered Workflow\\n\\nThis is the workflow I\'ve settled on after years of flipping levers and seeing what works (and what doesn\'t). Mandates don\'t work. Making analytics teams happy at the expense of poor application code going into production doesn\'t work. Knowing instrumentation is bad only after it is deployed works-ish, but just barely. Would not recommend.\\n\\n### Draft, iterate on, and deploy a schema.\\nThe neat thing about schema-first workflows is **non-engineer stakeholders can write the first draft**. You don\'t have to be a Typescript guru to get the process going, though engineering counterparts will need to be involved eventually.\\n\\nThe more work that can be front-loaded to non-engineers the better. Everyone\'s time is valuable and schemas allow everyone to proactively contribute to the process. It sucks when useful contributions are discarded because they are \\"not in a usable format\\" (cough, Gdocs). Schemas are usable formats.\\n\\n### Bring tracking libraries and systems up to parity.\\nThe second a new schema or updated version has been published, automation kicks in and (at minimum):\\n\\n- Builds and deploys new tracking SDK\'s for engineering teams\\n- Pushes schema metadata \u2206 to data discovery tools\\n- Ensures infrastructure dependencies (Kafka topics, database tables, etc)\\n- Pushes the schema to the appropriate place for pipeline-level validation\\n- Creates dbt sources for the analytics engineers\\n\\n### Implement tracking.\\n\\nOnce systems are ready to accept the new instrumentation, engineers implement it into a codebase. It doesn\'t matter if this is frontend code, backend code, cli\'s, infrastructure tooling, etc - the process is the same.\\n\\nGetting dependencies squared away takes a matter of seconds. By the time engineers are ready to implement the new tracking, the entire system is ready to go.\\n\\nA question I\'ve heard over and over from engineers is \\"how do I know these payloads are making it to where they need to be?\\" This question is best answered with \\"go check Datadog/Graylog/Whatever\\". And followed up with, \\"or you could also go check Snowflake for a table of the same name\\".\\n\\nThe faster engineers have feedback the better.\\n\\n### Deploy.\\n\\n\\nAnd lastly - making tracking part of the codebase. A huge pain point of analytics-oriented instrumentation is the fact it\'s often identified as \\"bad\\" after being pushed into prod. This is not awesome, and it greatly contributes to the upstream \\"we\'ll just throw arbitrary json down the line\\" concensus. Everyone knows this is not ideal, but it\'s definitely better than rolling back every-other deploy due to analytics bugs.\\n\\nWith contract-powered workflows the following prereqs are taken care of *before* instrumentation rolls out, not after:\\n\\n- Implementers and stakeholders talk to each other using shared verbiage.\\n- Versioned, language-specific data structures are generated like all other code dependencies.\\n- Metadata is pushed to discovery tools.\\n- The pipeline is primed to accept incoming payloads and mark them as \\"good\\" or \\"bad\\".\\n- Observability tools are ready to go for instantaneous feedback in development and production.\\n- Downstream analytics/modeling entrypoints (like dbt sources) are in place and can be immediately used.\\n\\n## The Schema-Powered Future\\n\\nIf it\'s not obvious by now, schemas are **awesome**.\\n\\nThese workflows have significantly improved my work life, I know they\'ve improved my colleagues\', and it\'s probably just a matter of time before they improve yours too.\\n\\nThe fun part is it feels like this ecosystem is just getting started, and there are **so** many additional implications for the better. It\'s not a new or original concept by any means. But as data management capabililities at Non-Google companies progress, it will be a consistent solution for consistent pain.\\n\\n**Some other reading if you want to dive in:**\\n\\n- [Data Wrangling at Slack](https://slack.engineering/data-wrangling-at-slack/)\\n- [Jitney at AirBnb](https://www.slideshare.net/alexismidon/jitney-kafka-at-airbnb)\\n- [Pegasus at LinkedIn](https://engineering.linkedin.com/blog/2020/pegasus-data-language)\\n- [Dragon at Uber](https://1fykyq3mdn5r21tpna3wkdyi-wpengine.netdna-ssl.com/wp-content/uploads/2020/03/Schema-Integration-at-Uber-Scale-US2TS-2020-1.pdf)\\n- [Building Scalable Real Time Event Processing with Kafka and Flink](https://doordash.engineering/2022/08/02/building-scalable-real-time-event-processing-with-kafka-and-flink/) at Doordash\\n- [Data Mesh at Netflix](https://netflixtechblog.com/data-mesh-a-data-movement-and-processing-platform-netflix-1288bcab2873)\\n- [Building a Real-time Buyer Signal Data Pipeline for Shopify Inbox](https://shopify.engineering/real-time-buyer-signal-data-pipeline-shopify-inbox)\\n\\n**You could also:**\\n- Tune in for [Getting jiggy with JSON Schema at dbt Coalesce](https://coalesce.getdbt.com/agenda/getting-jiggy-with-jsonschema-the-power-of-contracts-for-building-data-systems) or [join the conversation](https://twitter.com/emilyhawkins__) on [Twitter](https://twitter.com/aerialfly).\\n\\n\\nAs closing context from a Shopify perspective, 9800+ schemas and 1800+ contributors (many of whom are not engineers) is a huge feat. As is deploying hundreds of schema-generated instrumentation blocks to thousands of robots around the world. The model works.\\n\\nHere\'s to our schema-powered future \ud83e\udd42."}]}')}}]);